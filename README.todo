To Do

* catch deliberate exits in manager, in order cleanly shutdown pyro

* 'finished' => 'success', 'failed' => 'failure', 'completed' => 'done'

* consider 'set -e' and 'trap' in task scripts.

* 'raise SystemExit(message)' instead of sys.exit(1)

* remote background tasks will fail if PNS_HOST is 'localhost'
  (which is only correct on the local machine)

* when initial f/c failed in the file-move example (real mode)
  system required a 'bump' for monitor to show the task failed.

* treat task groups names just like task names.

* rewrite configure script in Python, for consistency.

* print error messages to stderr: print >> sys.stderr, 'message'

* think about task launch: qsub, loadleveller, other machines
  (queue and machine name should be task-specific?)

* 'cylc message --all-outputs-completed' is confusing because it does not 
  relate to the special 'completed' messages.

* consider implementing "either or" and "self destruct"
  prerequisites (Phil's idea) - this would allow automated
  control-system initiated response to failed tasks.  EXAMPLE: if a
  weather forecast fails, run a backup forecast configured with a
  shorter timestep; if the original forecast succeeds, the backup can
  self destruct. The forecast post-processing task(s) would trigger off
  either the original or the backup forecast.

* can a system that's not being monitored stall (i.e. never run through
  the task processing loop)?  This seemed to happen ONCE to the topnet
  test system while I was in the UK recently. Jan 2009: possible cause
  of this: if a task with no downstream dependencies fails
  (topnet_cleanup), the rest will carry on until delayed at the max
  maximum runahead time which results in a bunch of tasks at the front
  end finished but not abdicated. When the failed task is killed no task
  messages will come in to stimulate the event loop, because the
  existing tasks are all finished, so the system needs a remote "bump"
  to get going again.
  22 Feb: this happened again without any failed tasks: topnet test
  system was in hibernation for 10 days and reawoke when I hooked up a
  new monitor process.

* parse all command-line input to check for valid cycle times 

* check that task output times do not exceed the registered run time

* exception handling for a graceful exit if cylc can't find a Pyro
  nameserver.

* allow cylc task proxies to kill their real external tasks at shutdown
  (and otherwise)?

* connection failure retry for dummy tasks, as is currently done in
  'cylc message'

* use exception handling as in task_manager.insert() to prevent remote
  control operations that throw an exception (e.g. due to mis-formatted 
  user input) from bringing the system down.

* when there are multiple finished tasks that can satisfy a new task's
  restart prerequisites, the one that actually satisfies the new task
  will be an essentially random choice (the first one that comes along). 
  This is OK because the only thing that matters is that at least one
  task can satisfy the restart dependency, then the new task calls the
  prerequisite satisfied. However, we could get tasks to record the ID
  of the satisfier task as well, for each prerequisite, and also to
  choose the latest task as satisfier if more than one can do it.

* exception handling in message script: retry and/or alert in case
  of failure to contact the control system

* python script for easy viewing of configured log files without having
  to know their location?

* the task-generator script should check that the user-defined task
  names do not clash with parent class or attribute names.

* user config for restricted start time (e.g. 06 UTC only) and other
  system-specific restrictions?

* remote setting of 'ready' state for artificially tied tasks (nztide)
  whose artificial tie has failed but whose real prerequisites are known
  to be satisfied

* when remote killing all waiting tasks at a ref time, check that (a)
  the system has moved on passed that time, and (b) none of the waiting
  tasks are contact tasks whose time has not come up yet.

* tasks with conditional outputs may have trouble with the "keep one
  finished task of each kind" procedure in kill_spent_tasks(). best to
  have multiple tasks instead? 

* detect when taskdef files are newer than task_classes file, and warn
  that use of configure-system may be required.

* use the custom clock everywhere (it knows about dummy mode).

* clean up the clock class with respect to dummy vs real time, and move
  it into task manager?

* retrofit proper exception handling throughout (current use is sporadic).

* task.quick_death should default to False? (as for the task 
  def key that maps to it, %COTEMPORAL_DEPENDANTS_ONLY).

* forecast tasks can no longer be 'quick death' because they depend on
  previous instance. Cylc should check for this. However, they could
  die as soon as their successor is satisfied. 

====================================
FUZZY PREREQUISITES (only for infrequent 'advanced' usage - e.g. hourly
TopNet in EcoConnect)

* allow mixed fuzzy and non-fuzzy prerequisites (currently have to 
  set identical fuzzy bounds to simulate the non-fuzzy case; see
  topnet.py).

* FUZZY MATCHING CURRENTLY ASSUMES AT MOST ONE COMPATIBLE OLDER FINISHED
  VERSION OF THE UPSTREAM TASK IS PRESENT, otherwise the match occurs
  with the first one found, whichever it is. This can fail if a system
  problem puts a hold on spent task deletion! E.g.: topnet and oper
  interface go on ahead when topnet_vis has failed (unlikely to happen
  though!)

* just before a task runs, try to re-satisfy fuzzy prerequisites in case
  a more up-to-date satisfier has shown up while the task was waiting
  for other prerequisites to be satisfied ... OR (better?!) don't try to
  satisfy fuzzy prerequistes until after all non-fuzzy ones have been
  satsified. 
