To Do

* Add NIWA Copyright to all files.

* flexible and consistent taskdef times for outputs, contact delay etc.

* is taskdef CYCLES sorted for order?

* combine file-move with userguide-2, add SCS suite to the examples
  (with real mode).

* better example system timing spec for real mode (taskdef env vars?)

* DO NOT DEFAULT TO LOCAL HOST FOR PYRO NAMESERVER HOST NAME - this will
  preclude automatic discovery of remote nameservers.

* optionally allow a pyro request handling timeout.

* check that ALL input, including that from remote insertion etc., is 
  validated for type (e.g. ID must be name%YYYYMMDDHH) before being
  passed into the program.

* some way of using registered system name in config module, e.g. for
  instance-specific tmpdir

* create a log class so we can store the main logging dir ...

* catch deliberate exits in manager, in order cleanly shutdown pyro

* consider 'set -e' and 'trap' in task scripts:
# ABORT IN CASE OF ANY ERROR, AND ALERT CYLC
# (this means we do not need to explicitly check for success of simple
# operations like directory creation, etc.
trap 'cylc-message --failed' ERR  
( does this double count for deliberate 'exit 1' instances?)

* allow task shortnames for control and question commands.

* 'raise SystemExit(message)' everywhere instead of sys.exit(1)?
* print error messages to stderr: print >> sys.stderr, 'message'

* remote background tasks will fail if CYLC_NS_HOST is 'localhost'
  (which is only correct on the local machine)

* when initial f/c failed in the file-move example (real mode)
  system required a 'bump' for monitor to show the task failed.

* consider implementing "either or" and "self destruct"
  prerequisites (Phil's idea) - this would allow automated
  control-system initiated response to failed tasks.  EXAMPLE: if a
  weather forecast fails, run a backup forecast configured with a
  shorter timestep; if the original forecast succeeds, the backup can
  self destruct. The forecast post-processing task(s) would trigger off
  either the original or the backup forecast.

* can a system that's not being monitored stall (i.e. never run through
  the task processing loop)?  This seemed to happen ONCE to the topnet
  test system while I was in the UK recently. Jan 2009: possible cause
  of this: if a task with no downstream dependencies fails
  (topnet_cleanup), the rest will carry on until delayed at the max
  maximum runahead time which results in a bunch of tasks at the front
  end finished but not abdicated. When the failed task is killed no task
  messages will come in to stimulate the event loop, because the
  existing tasks are all finished, so the system needs a remote "bump"
  to get going again.
  22 Feb: this happened again without any failed tasks: topnet test
  system was in hibernation for 10 days and reawoke when I hooked up a
  new monitor process.

* check that task output times do not exceed the registered run time

* allow cylc task proxies to kill their real external tasks at shutdown
  (and otherwise)?

* improved exception handling and retry and/or alert in case in message 
  script and dummy tasks?

* use exception handling as in task_manager.insert() to prevent remote
  control operations that throw an exception (e.g. due to mis-formatted 
  user input) from bringing the system down.

* when there are multiple finished tasks that can satisfy a new task's
  restart prerequisites, the one that actually satisfies the new task
  will be an essentially random choice (the first one that comes along). 
  This is OK because the only thing that matters is that at least one
  task can satisfy the restart dependency, then the new task calls the
  prerequisite satisfied. However, we could get tasks to record the ID
  of the satisfier task as well, for each prerequisite, and also to
  choose the latest task as satisfier if more than one can do it.

* python script for easy viewing of configured log files without having
  to know their location?

* the task-generator script should check that the user-defined task
  names do not clash with parent class or attribute names.

* user config for restricted start time (e.g. 06 UTC only) and other
  system-specific restrictions?

* remote setting of 'ready' state for artificially tied tasks (nztide)
  whose artificial tie has failed but whose real prerequisites are known
  to be satisfied

* when remote killing all waiting tasks at a ref time, check that (a)
  the system has moved on passed that time, and (b) none of the waiting
  tasks are contact tasks whose time has not come up yet.

* tasks with conditional outputs may have trouble with the "keep one
  finished task of each kind" procedure in kill_spent_tasks(). best to
  have multiple tasks instead? 

* detect when taskdef files are newer than task_classes file, and warn
  that use of configure-system may be required.

* use the custom clock everywhere (it knows about dummy mode).

* clean up the clock class with respect to dummy vs real time, and move
  it into task manager?

* retrofit proper exception handling throughout (current use is sporadic).

* task.quick_death should default to False? (as for the task 
  def key that maps to it, %COTEMPORAL_DEPENDANTS_ONLY).

* forecast tasks can no longer be 'quick death' because they depend on
  previous instance. Cylc should check for this. However, they could
  die as soon as their successor is satisfied. 

====================================
FUZZY PREREQUISITES (only for infrequent 'advanced' usage - e.g. hourly
TopNet in EcoConnect)

* allow mixed fuzzy and non-fuzzy prerequisites (currently have to 
  set identical fuzzy bounds to simulate the non-fuzzy case; see
  topnet.py).

* FUZZY MATCHING CURRENTLY ASSUMES AT MOST ONE COMPATIBLE OLDER FINISHED
  VERSION OF THE UPSTREAM TASK IS PRESENT, otherwise the match occurs
  with the first one found, whichever it is. This can fail if a system
  problem puts a hold on spent task deletion! E.g.: topnet and oper
  interface go on ahead when topnet_vis has failed (unlikely to happen
  though!)

* just before a task runs, try to re-satisfy fuzzy prerequisites in case
  a more up-to-date satisfier has shown up while the task was waiting
  for other prerequisites to be satisfied ... OR (better?!) don't try to
  satisfy fuzzy prerequistes until after all non-fuzzy ones have been
  satsified. 
