_______
GENERAL

* config(suite) writes to stdout/stderr (gcylc parent terminal) - see
  gcylc:view_log() for a non-validating suite.

* document lockserver hosting issues: on a global filesystem, could use
  single lockserver across multiple machines (have to specify
  CYLC_LOCKSERVER_HOST in all suites, however) OR allow a separate
  lockserver on each host - but this won't prevent running the same
  suite on different hosts at the same time, which would cause probs in
  a global filesystem.  ALSO _THIS functionality is currently disabled
  in cylc -it is assumed that CYLC_SUITE_HOST is also the lockserver host.

* allow $CYLC_DIR/conf overrides by user in .cylc?

*  suicide prerequisites in cylc-3?

* reg export, copy, import should take group: as target even for single
  suites.

* clean up src/gui/textload.py and use of it: it was quickly hacked out
  of the tailing logviewer and should be treated separately and more simply.

* note DO NOT USE a suite reg GROUP:NAME as a suite definition dir
  as the colon screws up the PATH to the suite bin directory.

* tasks that are not in the dep graph can now be run with 'submit';
  should we allow insertion to?

* clean up prerequisites classes - put all in one module? (same with
  outputs + requisites?)

 * FOR SUITES WITH OWNED TASKS:
 replace suite owner with task owner in job submission log directory
 (e.g. for 'sudo -u owner llsubmit task' the job log dir ends up in the
 ll directives for stdout and stderr output; these must be writeable by
 the owner or the job will be put on hold by loadleveler). CURRENTLY
 this is hardwired into ll_basic.py job submission but needs to be made
 more generally applicable. Also, can we provide a general mechanism for
 owned tasks in case suite and task owner home directories are of
 different structure? Perhaps owned tasks need to set a job log
 directory in their suite.rc task section?

* job submission log directory for tasks with owners: MUST be relative
  to $HOME, which is used as loadleveler @initialdir, not absolute, or 
  the loadleveler stdout and stderr output file directives will end up
  pointing to the suite owner's space and the job will hang. Need a more 
  transparent way of doing this!

* raise SystemExit() in config.py, cylcconfigobj, (and anywhere else?) NO GOOD FOR GUI!

* chooser: setting group color fails if multiple suites are running.
  Need to parse each group member and check if any are running.

* Consider possible changes to cylc internals in light of the fact
  that as of cylc-3 we know all task dependencies in advance, thanks
  to the suite.rc dependency graph.  E.g. we may be able to remove many
  finished tasks ahead of the generic cleanup cut off.

* Consider: instead of keeping finished tasks around, maintain a
  database of finished outputs from which old data could "roll off"
  at said cutoff. This would probably have a performance advantage 
  for very large suites because it would eliminate the large numbers of
  finished tasks that potentially have to stay in the task pool
  for a while. This database would have to be written to file with every
  change with the state dump file, to allow valid suite restarts.

* suite db locking: document, and provide info via command response, 
  what to do in the unlikely event that a lock is in place when a
  transaction is attempted. Also, should trap Ctrl-C => unlock.

* locking code could be simplified or cleaned up a bit now that
  the lockserver is user-specific?

* click on a task and tell suite to pause upon completion thereof.

* Document: if a suite is restarted on a new port the gui still
  reconnects.

* inlined suite.rc: avoid comments ending in line continuation marker
  (2D textual dependency graph in comments at start of e.g. suite.rc)

* DOCUMENT: cylc.rc: BARE STRINGS ARE OK ONLY IF THEY DON'T CONTAIN
  COMMAS, which makes them look like lists to the configobj validator.

* document insertion danger: if you insert task A(T+N) ahead of A(T),
  the suite will shut down if A(T) catches up with A(T+N): 'task has 
  already registered its outputs'

* consider calling task-started from task execution script instead of
  cylc-wrapper or task scripts: this would eliminate submitted state
  failures, and allow error trapping on tasks with significant
  in-suiterc scripting (and scripting-only tasks?)

* 'coldstart tasks' could be replaced with a list auto-determined
  by parsing the dependency graph?

* document that restarting early on in a coldstart will not be
  recognized as a coldstart.

* currently all task reset commands go through the 'remote' object,
  which all suites have, but we could get a proxy for the task proxy directly,
  in which case we'd could catch Pyro.NamingError to detect if no such
  task exists in the suite. 

* Pyro config:
  + PYRO_MAXCONNECTIONS , default 200: max number
  of simultaneous connections to a single pyro server?
  The default connection validator checks for this.
  + port range: set sufficiently wide for UKMO.
  (Consider cylclockd too)
  + consider Pyro multi-threading again?
  + use passphrase authentication in the default pyro connection validator
 (see the Pyro userguide security page).
  THESE SHOULD BE USER-SPECIFIC GLOBAL PREFERENCES?

* allow single tasks to be paused ?

* '--host' should not be allowed for intervention commands because
  owner's username does not cross machine boundaries, generally.

* consider the need for different spawning behaviour of sequential (and
  tied?) tasks - why not just depend on prev%finished?

* document that continuation lines and include-files are cylc-specific
  extensions to ConfigObj.

* explain conditional prerequisites (and suicide ones...) in requisite dumps.

* put roadmap into documentation. 

* generate new stdout and stderr log files when a task is reset (done?).

* in the new config format, what happens if we restart after suite.rc
  tasks changed?

* TASK FAMILIES:
  + Ability to reset (or otherwise operate on) all members of a family.
  + Check effect of 'reset to waiting' on task families - need to reset
    the special finished prerequisites too?
  + Task family needs to be set 'failed' if all (or any?) members fail
  + use with --include and --exclude?
  + task owner should not be required with families

* IMPORTANT: check that the user-defined task names do not clash with
  parent class or attribute names.

* at some point, do some profiling of execution time and memory use:
    cProfile
    hotshot
    timeit
  are all part of the standard library. The first (and hostshot too?)
  generate stats files that can easily be manipulated and viewed
  with the pstats module.  See standard Python documentation, and
  this article:
    http://pparkkin.info/2010/02/21/profiling-python-hotshot-timeit-and-heapy
  For finding any memory leaks, see:
     heapy
  (not std lib, but explained in the above article).

* --port= does not prevent port scan for lockserver?
  check for non-essential port scanning, esp. in gui.
  (see GUI note below)

* document how to re-run partial suite over existing data:
    - exclude some tasks
    - insert a new task to generate the outputs representing
      the existing data, and/OR
    - dummy out the tasks that generated the existing data.

* inserting an already existing task causes cylc to shut down with 
  'error: task X has already registered its outputs' - is this the
  desired behavior? (FIXED?)
* can doing task deletion and insertion while a suite is PAUSED result
  in "INSERTING A TASK
      ERROR: A%2010081800 has already registered its outputs"
(DUPLICATE; see above)

* document reason for naming of pyro-connected objects
  (owner.suite.obj): prevent accidentally intervening in the wrong
  suite when using explicit port numbers (if we allow that). Also, 
  is this still required?

* temporary job submission filenames should contain the suite name
  so that automated cleanup processes can distinguish between suites.

* check use of re m.group() and m.groups(): 

    m.group(0)  # entire match
    m.group(1)  # first parenthesised sub-group
                # ...

    m.groups()  # tuple of parenthesised sub-groups


* task ID should not be needed in message strings (?); it can be supplied 
  automatically by access methods (for started, finished, etc.)

* cycle-dependent number of restarts: this can cause problems because
  only the most recent finished task is retained to satisfy
  prerequisites. If a task alternately does short and long forecasts
  with accordingly few and many restart outputs, we must retain *both*
  previously finished short and long versions of the task when currently
  only the short one would be retained. THE CYLC TASK ELIMINATION
  ALGORITHM MUST TAKE RESTART OUTPUTS INTO ACCOUNT: Do not eliminate a
  finished task if its restart outputs are still valid.
* DUPLICATE?: consider restart messages for split tasks that actually
  use the same restart files: e.g. nzlam_long (06,18Z) and nzlam_short
  (00,12Z).  Currently must combine this into one task with
  conditionals, OR register (and report) restart messages explicitly
  because the automatic restart messages will not have the right task
  name.

* some kind of internal queuing system as a more flexible alternative to
  the simple max runahead limit?

* commandline option parsing: defaults can be set to int or float, but 
  values read from commandline input are always strings and need to be
  converted.

* dummy mode restart: clock-rate is set by state dump file, so no point 
  in having rate as a commandline option unless it overrides. (and what
  about offset? should it be in the state dump too?).

* a new task type modifier that allows failed "non-critical" tasks to be
  ignored when applying the max runahead limit?

* dot-file graph output assumes that task families are internally
  cotemporal - is this necessarily the case? (is this still true - since
  rewritting the graphing code to use pygraphviz?)

* check all open( file, mode ) statements for mode 'rb' etc. 

* wherever I've used this:
    os.makedirs( os.path.dirname( filename ) )
  check that filename is not just a bare local dir filename like foo.bar
  as opposed to dir/foo.bar, else dirname() will return empty string and 
  makedirs() will fail.  This could happen if the user tries to use
  files in $PWD without specifying the full path.

* example system implementations should always use $USER in temp dir
  paths, to avoid interference between users. (done?)

* allow default loadleveler directives for ll_basic etc., to be
  specified in suite.rc?

* allow insertion of tasks in 'spawned already' state, for oneoff runs 
  of non-oneoff tasks?

* purge algorithm: see TO DO in the in-method comments: ensure that ALL
  tasks whose prerequisites get satisfied in the virtual system
  evolution that occurs in the purge algorithm, get unsatisfied again at
  the end of it.

* practice mode should be allowed even for exclusive suite locks. 

* stop (without --now) won't cause shutdown if a task family (but not
  it's members!) has just triggered.

* task state at initialization: there's no point in giving initial
  state of "finished" for example, if this does not result in setting
  prerequisites and outputs satisfied and completed, respectively.
  Consolidate this task state resetting code into methods in task.py?

* note that prerequisites are no longer derived from requisites. Change
  documentation in the latter class (and elsewhere?) to reflect this.

* a minor redesign of jobsumit / launcher class to avoid the current
  clunky method of setting some class parameters directly instead of by
  instance initialization or method call? e.g. job_submit.env = env.

* --fail=TASKID: check that TASKID exists in the target suite.

* In task scripts, document that failure inside RES=$( foo.sh ) does not
  trigger the err trap!

* document new use of 'TaskID failed' as an output: added on the fly as
  a new output if failure occurs, then removed from the output list on
  reset.

* use new cylc_mode class everywhere mode test is required.

* cylc submit --scheduler: task state resets to 'running' but all
  messages are ignored.

* extreme task elimination method - extrapolate forward in time to see
  if a finished task will ever be needed, otherwise delete?

* iteration over dictionary items: DO NOT USE
  = for item in dict:
  =    (operation that adds or removes items from dict)
  -> RuntimeError: dictionary changed size during iteration

  Instead use
  = for item in dict.keys():
  which must build a temporary list?

  CHECK USE OF DICTIONARY ITERATION THROUGHOUT THE CODE

* src/which.py, like the shell command, searches only for executable
  files. The ll_raw job submit classes use which to find task scripts
  that don't have a full path supplied in the taskdef; these need
  to check that which returns a result before continuing (e.g. if 
  the task script has not been set executable). 

* daemon and asynchronous task: initial tests done; go back, check, and
  complete, for new versions of cylc.

* message command (command line usage only) lock and practice mode?

* consider reset --no-spawn (Bernard tried to reset a waiting unspawned
  task to finished, which made it spawn ... is this the desired
  behaviour?)

* check that practice mode suites initialise with the same clock time
  as the original suite!

* DOCUMENT or CHANGE: cotemporal peers of failed tasks are not deleted
  automatically because we USED TO restart with failed tasks in the 
  'waiting' state (not 'ready') - thus the aforementioned peers may be
  required to satisfy the failed task's prerequisites post resetting.

* Consider the effect of "# uncomment for earliest NON-FAILED" (x2) in
  manager.py - and test with dummy mode failout tasks.  For failed F in
  userguide example suite, it allows some intermediate finished cycles
  to be deleted, but this does not affect the delay due to max runahead.

* consider task proc loop invocation - could we separate summary update
  vs task processing (e.g. when a task fails we have to run pointlessly
  (?) through the loop in order to update the summary immediately for
  monitoring.

* document this: deleting a task may allow the suite to move on to the
  point that a reinserted task will not get its prerequisites satisfied
  automatically.

* consistent suite exit strategy: 
   -sys.exit(1), 
   -raise SystemExit(message)
   -custom exceptions

* all error messages should go to stderr: print >> sys.stderr, 'message'

* allow cylc task proxies to kill their real external tasks at shutdown
  (and otherwise)?

* make sure that no remote operation, other than 'stop', can bring a
  suite down (exception handling on all remote switches).

* when there are multiple finished tasks that can satisfy a new task's
  restart prerequisites, the one that actually satisfies the new task
  will be an essentially random choice (the first one that comes along). 
  This is OK because the only thing that matters is that at least one
  task can satisfy the restart dependency, then the new task calls the
  prerequisite satisfied. However, we could get tasks to record the ID
  of the satisfier task as well, for each prerequisite, and also to
  choose the latest task as satisfier if more than one can do it.

* when remote killing all waiting tasks at a ref time, check that (a)
  the suite has moved on passed that time, and (b) none of the waiting
  tasks are clock-triggered tasks whose time has not come up yet.

* we need a way to make the the custom clock (accelerated in dummy mode)
  available throughout the code, so we can easily use it anywhere
  (task.set_execution_timer is a kludge we could avoid, for instance).

* clean up the clock class with respect to dummy vs real time, and move
  it into task pool?

* do we need non-interpolation of single-quoted strings in taskdef
  environment, scripting, command? (implement in job_submit.py)?

* see To Do comment in broker.negotiate() - priority low.

* execute.execute( [command]) blocks until command completes: THIS MAY
  NOT BE GOOD FOR TASK FAILURE HOOK SCRIPT INVOCATION?

__________
LOCKSERVER

* allow for a different lockserver host? (probably not). 

* document that when a suite releases its lock all of that suite's task
  locks are also released (e.g. when a suite is stopped --now with 
  tasks still running).

___
GUI

* dynamic reloading of the LED panel if the number of tasks in the
  running suite change (currently we have to start with *all* tasks
  including unused ones listed.

* don't parse suite.rc if the suite is already running?

* currently scans for port on every command - should store found port.

* access to task stdout and stderr for remote tasks? In fact, more
  generally, current we assume the viewer is running locally.

* VIEWING AND INTERACTING WITH REMOTE SUITES:
THIS IS PROBABLY NO LONGER RELEVANT SINCE DITCHING THE PYRO NAMESERVER
  'cylc view --host=foo'               .... works
  'cylc view -u USER --host=foo SUITE' .... fails
  The '-u USER' option does not work for cylc commands when '--host' is
  required (remote machine) because cylc preferences looks for USER's
  home directory locally => WHEN --host IS USED WE CANNOT LOAD PREFS (TO
  PRELOAD THE TASK LIST; HAVE TO GET IT DYNAMICALLY FROM THE RUNNING
  SUITE).  

====================================
FUZZY PREREQUISITES (only for infrequent 'advanced' usage - e.g. hourly
TopNet in EcoConnect)

* allow mixed fuzzy and non-fuzzy prerequisites (currently have to 
  set identical fuzzy bounds to simulate the non-fuzzy case; see
  topnet.py).

* FUZZY MATCHING CURRENTLY ASSUMES AT MOST ONE COMPATIBLE OLDER FINISHED
  VERSION OF THE UPSTREAM TASK IS PRESENT, otherwise the match occurs
  with the first one found, whichever it is. This can fail if a suite
  problem puts a hold on spent task deletion! E.g.: topnet and oper
  interface go on ahead when topnet_vis has failed (unlikely to happen
  though!)

* just before a task runs, try to re-satisfy fuzzy prerequisites in case
  a more up-to-date satisfier has shown up while the task was waiting
  for other prerequisites to be satisfied ... OR (better?!) don't try to
  satisfy fuzzy prerequistes until after all non-fuzzy ones have been
  satsified. 
