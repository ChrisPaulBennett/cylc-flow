
=============== ECOCONNECT CONTROLLER WITH IMPLICIT SEQUENCING =============

Model Sequencing ----------

The EcoConnect forecasting engine consists of a set of tasks that must
be executed in correct order as determined, mainly, by availability of
input files that are generated by other tasks (a "task" is any set of
processes that, as a group, we want separate scheduling control over;
this includes, but is not restricted to, "a science model plus its
post-processing"). In general a task may take input from more than one
upstream task, and may generate output for more than one downstream
task; this results in a non-linear task "sequence" that consists of
parallel streams of execution that sometimes branch and merge.


Explicit Sequencing ----------

This is the obvious method to try first: determine the final task sequence in
advance, taking all task interdependencies into account, and write code to
explicitly enforce the correct sequencing. This can be coded as some kind of
finite state machine, which is relatively easy to understand ("if A is
finished and B is finished, then run C" etc.), but the program will
inevitably(?) be fragile and inflexible with respect to insertion and deletion
of models, mid-stream startup, maintenance and extensibility, etc,

Implicit Sequencing ----------

Concept (according to Hilary Oliver):

Rather than enforcing correct sequencing from the outset, get the tasks to
interact in such a way that correct sequencing emerges naturally at run time.

Each task has its own set of prerequisites that have to be satisfied before it
can run (e.g. "input file foo is ready") and "postrequisites" that are
satisfied as it runs (e.g. "output file bar is ready"). This is the minimum
external interface information needed to run a task, even in standalone mode,
BUT it actually entirely determines the global task sequence too: i.e. no
"sequencing logic" is needed because that information is encoded in the
matching of one task's postrequisites with another's prerequistes. To make use
of this, however, we have to get the tasks to interact at run time.

Implementation

Tasks are represented within the controller by TASK OBJECTS that know their
own requisites, can interact with others to get their prerequisites satisfied,
can launch their external tasks as soon as their prerequisites are all
satisfied, and whose internal states are kept in sync with their external
tasks (e.g. notification of completed postrequisites) by a messaging system
(Pyro, the Python Remote Object protocol, allows direct communication between
external tasks and their task objects in the controller). 

Note that task objects do not need to know *who* will satisfy their
prerequisites as they can inquire of all the other tasks (this is a great
simplification). Note also that pre and postrequisites are just *messages*,
e.g. an external task has started or finished, or a particular file is ready
for use. In the case of "file ready" messages, the controller does not need to
know the actual location of the file. It could easily be made to, but there's
not really any point in doing so because the external tasks have to know that
anyway (at least within EcoConnect) and they can report back any "file not
found" errors.


Description of The Main Algoritm:

Nomenclature:

A task object is uniquely identified by its type, e.g. 'nzlam', and its
associated reference time: foo(T) is "an object of task type foo at time T". 
foo(T+1) means "an object of task type foo at the NEXT reference time,
relative to foo(T), that is valid for task type foo".

EXACT PREREQUISITES (most tasks): times are specified exactly, relative to
the task's own reference time.  E.g. "file foo_{T}.nc ready" where T is the
task's reference time.

FUZZY PREREQUISITES (topnet): a time boundary, Tb, is specified relative to
the task's own reference time, and any task with a reference time greater than
or equal to Tb can satisify the prerequisite.

Algorithm:

+ the user supplies a list of task names and an initial reference time.
    + a task object is created for each named task at the start time, OR at
    the next valid reference time for that task type
    + tasks are created in a 'waiting' state by default.

+ thereafter, new tasks are only created by ABDICATION: foo(T+1) is created
only when foo(T) reaches a 'finished' state.
    + foo(T) isn't deleted as soon as foo(T+1) is created, however, (see below) 
    + this ensures that foo(T+1) can't run before foo(T), even if foo has no
    prerequisites (and can therefore run as soon as the task exists).  

+ initially the task processing block (below) is run through once so that any
tasks with no prerequisites (e.g. the 'Met Office downloader') will launch
their external jobs and enter the 'running' state immediately.

+ the program then enters the PYRO REQUEST LOOP, which:
    + handles incoming remote method calls from external tasks.  
    + returns after one or more remote method calls are handled. 

+ the TASK PROCESSING BLOCK runs whenever the Pyro request loop returns after
handling one or more task messages (i.e. one or more tasks changed state): 

    1. REGENERATE: create new foo(T+1) if foo(T) has finished

    2. INTERACT: each task asks all the others, "can you satisfy any of my
       unsatisfied prequisites with one of your completed postrequisites"?

    3. RUN_IF_READY: tasks are told to run (and enter the 'running' state) if:
        + their prerequisites are all satisfied
        + any previous tasks of the same type that still exist are 'finished' 
        + fewer than MAX_RUNAHEAD=4 finished tasks of the same type exist
        (this stops tasks with no prerequisites, e.g. nztide, from running
        ahead indefinitely).

    4. DUMP_STATE: the current state (waiting, running, or finished) of all
    tasks in the system is written out to the "state dump file".

        + this provides a snapshot of the system just prior to shutdown.

        + the controller can be initialised from the state dump file

            + the state dump file can be edited before reloading

            + any 'running' tasks are reloaded in the 'waiting' state.

    5. KILL_SPENT_TASKS: a task is spent if it (a) finished, and (b) no longer
       needed to satisfy the prequisites of any other task.

       + each non-finished task reports its CUTOFF REFERENCE TIME, i.e. the
       reference time that could still be needed for satisfying dependencies.
       In most cases this is just the tasks own reference time. For hourly
       topnet, it is the reference time of the previous finished 06 or 18Z
       'nzlam_post' task (the next hourly topnet may get input from the same
       nzlam_post).  

       + the task manager then kills any batch of cotemporal tasks that are
       all finished AND older than the oldest task cutoff time.

    6. KILL_LAME_DUCKS: a lame duck is a task that will never run because its
       prerequisites can not be satisfied by any other task in the system. 
       They need to be removed or they'll prevent the spent task deletion
       algorithm from working. 
       
       + lame ducks can only be detected in the OLDEST batch of tasks(T). At
       other times new tasks that could satisfy some dependencies could
       potentially still appear as their predecessors abdicate.

       + the presence of lame ducks may indicate user error: e.g. forgetting
       to include task type "foo" that supplies input to task type "bar" will
       turn any instance of "bar" into a lame duck.

       + lame ducks are to be expected in certain start up situations. E.g. if
       we start the system at 12Z with topnet turned on, all hourly topnet
       objects from 12Z through 17Z are valid, but they will want to take
       input from the non-existent nzlam_post from 06Z prior to startup.

       + lame ducks are abdicated rather than just deleted, because on of
       their descendents may not be lame (see topnet case above). 


COMMENTS:

The algorithm above achieves all of our requirements (multiflight, catchup,
flexibility, etc.) even for the unusual "fuzzy runahead" behaviour of topnet,
while still being completely generic: the main program knows nothing at all
about task sequencing, it simply operates on a single pool of tasks that know
only of their own individual prerequisites and outputs. How to do this was by
no means obvious at the outset, however, because immediately below the
high level implicit sequencing concept there is almost too much flexibility in
terms of how to "manage" the task objects.  Below are some notes on task
management that help to explain how I arrived at the above algorithm.


*** Generality:

Correct sequencing (i.e. that no task will run before its prerequisites are
satisfied) will occur regardless of the task management scheme used, so long
as tasks exist by the time they are needed. 

But, that said, we still have to come up with a specific task management
scheme that does ensure tasks exist when they are needed, and also, ideally,
that tasks don't exist for too long before they are needed or for too long
after they are no longer needed, because having too many tasks around longer
than necessary makes system monitoring difficult.

*** How to control when a task executes:
 (i)   prerequisites
 (ii)  artificial prerequisites (e.g. make nztide depend on nzlam)
 (iii) delayed instantiation (a task can't run if it doesn't exist yet).
 (iv)  other contraints based on, for example, the number of previous instances
       that still exist in the system.


*** The INITIAL Task: 

At least one task object, at system start up time, must have no prerequisites
so that it can start running immediately, otherwise nothing will run at all.
The downloader task is of this type. It will immediately launch its external
task, which will go off and wait for the right files to come in, reporting
back when the download has completed OR immediately if the files already exist
because they were downloaded at some earlier time.

*** INTER-CYCLE DEPENDENCIES:

Having each task foo(T) depend on its previous instance foo(T-1) via explicit
prerequisites prevents successive instances of the same task from running
simultaneously or out of order (where their other prerequisites would allow
this; e.g. downloader or nztide) but it complicates system startup (when there
are no previous tasks) and means finished tasks have to stay around longer (so
that foo(T-1) can tell foo(T) it is finished). 

Instead of using explicit prerequisites between cycles, we can let a task run
if all previous instances that still exist are in the 'finished' state, OR if
no previous instances exist.  This still prevents running out of order but
system startup is no longer special, and tasks can (usually; see below) die as
soon as all of their same-cycle peers are finished.


*** DETECTING CATHCUP MODE

Catchup vs up-to-date operation is now task-dependent, rather than a property
of the whole system (as it is if only tasks from the same reference time can
run at once).  Where this matters, it can be detected by the relevant external
task. E.g. if the external topnet(T) task starts up at real time t greater
than the streamflow data time for T (i.e. T+15 min), i.e. the required
streamflow data is already available, then we're still in catchup. If, on the
other hand, the topnet task finds that it has to wait for its streamflow data
time to arrive, then we're caught up to real time.  This matters because
topnet is allowed to run ahead by a different amount of time depending on
whether we're in catchup mode or not.



DUMMY MODE

The controller has a "dummy mode" that allows *almost* complete testing of the
control system without running any external tasks/models at all. In dummy
mode, each task object launches an external dummy program instead of the real
external task. The dummy program interrogates its representative task object
in the controller (via Pyro) to find out what its postrequisites are, then it
reports each one satisfied in turn at the (estimated) right time relative to
an accelerated clock, i.e. dummy programs complete in approximately the same
dummy clock time as the real tasks do in real time. User's can also specify an
initial dummy clock time offset relative to the starting reference time, which
allows us to simulate catchup mode and real time operation, and the transition
between. Log messages are stamped with dummy clock time instead of real time.

As far as the controller is concerned the only (?) differences between dummy
mode and real operation is that the external dummy tasks do not suffer from
delays due to resource contention. 

The same "dummy task program" is used for all external dummy tasks, but it has
special behaviour when representing the downloader or topnet tasks, in order
to correctly simulate the behaviour of the corresponding real tasks: the
downloader "waits" for incoming files until the clock time is at least 3:15
past its reference time, and topnet "waits" until the clock time 0:15 min past
its reference time, which is the stream flow data extraction cutoff.

The dummy clock can be bumped forward by a number of hours, by remote control,
while the system is running. This affects the postrequisite timing of running
tasks correctly, but if it causes a running task to finish immediately the
next task in line will still start from the beginning no matter how big the
bump.


SINGLE THREADED PYRO REQUIRED

In SINGLE THREADED PYRO, handleRequests() returns after EITHER a timeout has
occurred OR at least one request (remote method call) was handled.  With
"timeout = None" this allows us to process tasks ONLY after remote method
invocations come in. Further, the processing_required boolean set in
task_base.incoming() allows us to process tasks ONLY when a task changes state
as a result of an incoming message, which minimizes non-useful output from the
task processing loop (e.g. in dummy mode there are a lot of remote calls on
the dummy clock object, which does not alter tasks at all). 

In MULTITHREADED PYRO, handleRequests() returns immediately after creating a
new request handling thread for a single remote object and thereafter remote
method calls on that object come in asynchronously in the dedicated thread.
It is impossible(?) to make our main loop work properly like this because
handleRequests will block until a new connection is made, even while messages
from existing remote objects are coming in.  Tasks that are ready to run are
only set running in the processing loop, so these will be delayed
unnecessarily until handleRequests returns.  The only way out of this is to do
task processing on a handleRequests timeout as well, which results in a lot of
unnecessary task processing."""
 

########### TO BE DOCUMENTED:

 1/ simple system monitors
 2/ remote control: 
    + clean shutdown of pyro
    + bumping the dummy clock forward
 3/ external task messaging interface script
 4/ config file, and optional initialisation from state dump
 5/ dummying out specific tasks in real mode


########### OTHER NOTES

Note that during CATCHUP operation "correct model sequencing" is not
equivalent to "orderly generation of products by reference time".  E.g. nzlam
can run continuously regardless of downstream processing that depends on it.

The task interaction step probably doesn't scale well with very large task numbers.
