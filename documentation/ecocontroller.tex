\documentclass[12pt]{article}

\title{Multiflight Dynamic Sequencing Controller Documentation}

\author{Hilary Oliver, NIWA}

\begin{document}
\maketitle

\section{EcoConnect Task Sequencing} 

The EcoConnect forecasting engine consists of many tasks\footnote{A {\em
task} is a set of processes that we want separate scheduling control
over, as a group.} that have to execute in the right order, which is
largely determined by availability of input files: one task's output is
input for the next task, and so on. In general this results in a complex
non-linear task ``sequence'' consisting of multiple parallel streams of
execution that can branch (when a task supplies input to several others)
and merge (when a task takes input from several others). The control
system that automates this needs to be flexible (allow tasks to be
switched on and off easily) and extensible (allow addition of new task
types easily) and it must allow {\em multiflight} operation - running
tasks from several forecast cycles at once, where dependencies allow -
for maximum throughput after system delays. 


\subsection{The Problem with Explicit Sequencing Logic}

The obvious control system solution is to determine the correct task
sequence in advance and write a Finite State Machine to enforce it:

\begin{verbatim}
IF [file foo.nc has been completed by task A] 
AND [task B is finished]:
THEN [run task C]
etc.
\end{verbatim} 

This is relatively easy to understand but it rapidly gets complicated
and the resulting program will almost inevitably be very inflexible. It
is also hard to see how to generalise it to multiflight operation
without ending up in a convoluted mess (note that we can't just fire off
a sequence of overlapping independent single-flight controllers because
of intercycle dependencies\footnote{The simplest of these is that
$foo(T+1)$ often depends on $foo(T)$: most forecast models start from a
``background state'' generated by the previous forecast.}). 


\section{Dynamic Task Sequencing}

This method was inspired by basic object oriented game programming
techniques. Tasks in the system are represented by objects that are
independent, but are able to interact to satisfy each others'
dependencies as the system runs, i.e. correct sequencing just emerges
naturally at run time. The total lack of explicit sequencing logic makes
this method extremely flexible: any task can be added to the system or
turned off at any time without modifying the control system at all.


\subsection{Implementation}

Each task has {\em prerequisites} that have to be satisfied before it
can run, e.g. ``file foo is ready'' [to use], and {\em postrequisites}
that are satisfied as it runs, e.g. ``file bar is ready'' [to be used by
others]. This is in fact just the minimum external interface information
needed to run a task, even in isolation, {\em but} it actually entirely
determines the global task sequence too, i.e. explicit sequencing logic
should not be needed because that is already encoded in the match-up of
pre and postrequisites. 

Tasks are represented by {\em task objects} that know their own
``requisites'' and can interact with others as described, launch their
external tasks when their prerequisites are all satisfied, and whose
internal states are then kept in sync with the external tasks (e.g.
notification of completed postrequisites) via a messaging system (Pyro,
the Python Remote Object protocol, allows direct communication between
external tasks and their representative objects in the controller). 

Note that task objects do not need to know {\em who} will satisfy their
prerequisites. They can just ask {\em all} the other tasks, which is a
nice simplification. Also, note that pre and postrequisites are just {\em
messages}: in the case of ``file ready'' messages, the controller does
not need to check for the file's existence because the external tasks
necessarily have to do that anyway, and they can report back any errors.


\subsection{The Algorithm}

There are many possible ways to implement dynamic sequencing, in terms
of task management within the controller. At one extreme, for example,
we could create a month's worth of tasks at once and just let them
interact until everything has run to completion.  That's refreshingly
simple, and nothing would run out of sequence, but it has some negative
consequences. For instance, system monitoring could be difficult, the
end of the month presents an artificial barrier to multiflight
operation, and all tasks with no prerequisites, for the whole month,
would want to run immediately.

The adopted algorithm is still quite simple but it is continuous, and
easier to monitor: task objects exist by the time they are needed but
not for too long before that, and not for too long after they are spent.
Note that the algorithm is completely generic in that it knows nothing
of relationships between the tasks it manages; it simply operates on a
single pool of tasks, each of which cares only about its own inputs and
outputs.  

\begin{itemize}
    \item At startup, the user supplies an initial reference time and
    list of task names. Task objects are created for each named task at
    the initial reference time, {\em or} at the first subsequent
    reference time that is valid for the particular task type.
    \begin{itemize}
        \item tasks are uniquely identified by type and reference time. 
        \item new task objects are created in a {\em waiting} state, by
        default.
        \item we can also start from a previous {\em state dump} file
        (see below).
    \end{itemize}

\item After startup, new tasks are created only by {\em abdication}:
foo(T+1) is created only when foo(T) achieves a {\em finished} state.
    \begin{itemize}
    \item this ensures that $foo(T+1)$ won't run before $foo(T)$ without
    resorting to explicit intercycle prerequisites that would require
    special treatment at startup (when there is no previous cycle). 
    \item it also ensures that tasks with no prerequisites, e.g.
    downloader and nztide, won't all try to run at once.
    \item tasks are not deleted immediately on abdication (see below). 
    \end{itemize}

\item An initial run through the {\em task processing block} (see below)
causes any tasks with no prerequisites (e.g. 'downloader') to enter the
'running' state and launch their external tasks immediately.
    \begin{itemize}
    \item if there are no tasks that lack prequisites, nothing starts running.
    \end{itemize}

\item The program now enters the {\em Pyro request loop}, which:

    \begin{itemize}
    \item handles remote method calls coming in from external tasks, 
    \item returns after at least one remote method call is handled. 
    \end{itemize}

\item The {\em task processing block} executes whenever the Pyro request
loop returns and one or more tasks have changed state: 

    \begin{itemize} 
    \item \verb#regeneration#: create new tasks by abdication.

    \item \verb#interaction#: satisfy each others' prerequisites. 

    \item \verb#run if ready#: launch your external task and enter the
    {\em running} state if:
        \begin{itemize}
        \item your prerequisites are all satisfied
        \item any existing older tasks of your type are {\em finished} 
        \item fewer than {\em MAX\_ RUNAHEAD} finished tasks of your
        type still exist (this stops tasks with no prerequisites from
        running ahead indefinitely).
        \end{itemize}

    \item \verb#dump state#: the current state (waiting, running, or
    finished) of all tasks is written out to the {\em state dump file}.
        \begin{itemize}
        \item this provides a snapshot of the system just prior to shutdown.

        \item the controller can be initialised from the state dump file
            \begin{itemize}
            \item the state dump file can be edited before reloading

            \item any 'running' tasks are reloaded in the 'waiting' state.
            \end{itemize}
        \end{itemize}

    \item \verb#kill spent tasks#: a task is spent if it (a) finished,
    and (b) no longer needed to satisfy the prequisites of any other
    task.
       \begin{itemize}

       \item each non-finished task reports its {\em cutoff reference
       time}, i.e. the oldest reference time that it thinks is still
       needed for satisfying its own, or its successors, prerequisites.
       In most cases this is just the tasks own reference time. For
       hourly topnet, it is the reference time of the previous finished
       06 or 18Z 'nzlam\_post' task (the next hourly topnet may get
       input from the same nzlam\_post).  

       \item the task manager then kills any batch of cotemporal tasks
       that are all finished {\em and} older than the oldest task cutoff
       time.

       \end{itemize}

    \item \verb#kill lame ducks#: these are tasks that will never run
    because their prerequisites can not be satisfied by any other task
    in the system.  

       \begin{itemize}
       \item lame ducks need to be removed or they'll prevent the spent
       task deletion algorithm from working.
       
       \item lame ducks can only be detected in the {\em oldest} batch
       of cotemporal tasks. At other times new tasks, which may satisfy
       someone's prequisites, may appear later on as their predecessors
       abdicate.

       \item the presence of lame ducks may indicate user error: e.g.
       forgetting to include task type ``foo'' that supplies input to
       task type ``bar'' will turn any instance of ``bar'' into a lame
       duck.

       \item lame ducks are to be expected in certain start up
       situations. E.g. if we start the system at 12Z with topnet turned
       on, all hourly topnet objects from 12Z through 17Z are valid, but
       they will want to take input from the non-existent nzlam\_post
       from 06Z prior to startup.

       \item lame ducks are abdicated rather than just deleted, because
       on of their descendents may not be lame (see topnet case above). 
       \end{itemize}
   \end{itemize}
\end{itemize}


\appendix

\section{Dummy Mode}

Dummy mode allows complete testing of the control system without running
any external tasks: the only difference from real operation, as far as
the controller is concerned, is that external dummy tasks are not
delayed by resource contention. 


In dummy mode, task objects launch an external dummy program instead of
real external tasks. The dummy programs get a list of postrequisites
from their parent task object (via Pyro) and report back that each one
is satisfied in turn, at the right time relative to an accelerated
clock, so they complete in approximately the same dummy clock time as
the real tasks do in real time. An initial dummy clock offset relative
to the initial reference time can also be specified, which also allows
simulation of catchup operation and the transition to real time mode.
Log messages are stamped with dummy clock time instead of real time.

The same script is used for all external dummy tasks but it has special
behaviour when representing some tasks (downloader and topnet) in order
to correctly simulate their behaviour (the downloader ``waits for incoming
files'' until the clock time is at least 3:15 past its reference time, and
topnet ``waits until the clock time 0:15 min past the task reference
time'' (i.e. the stream flow data extraction cutoff).

The dummy clock can be bumped forward by a number of hours by remote
control, while the system is running. This affects the postrequisite
timing of running tasks correctly, but if it causes a running task to
finish immediately the next task in line will still start from the
beginning no matter how big the bump.


\section{Threading in Pyro}

In SINGLE THREADED PYRO, handleRequests() returns after EITHER a timeout has
occurred OR at least one request (remote method call) was handled.  With
``timeout = None'' this allows us to process tasks ONLY after remote method
invocations come in. Further, the processing\_required boolean set in
task\_base.incoming() allows us to process tasks ONLY when a task changes state
as a result of an incoming message, which minimizes non-useful output from the
task processing loop (e.g. in dummy mode there are a lot of remote calls on
the dummy clock object, which does not alter tasks at all). 

In MULTITHREADED PYRO, handleRequests() returns immediately after creating a
new request handling thread for a single remote object and thereafter remote
method calls on that object come in asynchronously in the dedicated thread.
It is impossible(?) to make our main loop work properly like this because
handleRequests will block until a new connection is made, even while messages
from existing remote objects are coming in.  Tasks that are ready to run are
only set running in the processing loop, so these will be delayed
unnecessarily until handleRequests returns.  The only way out of this is to do
task processing on a handleRequests timeout as well, which results in a lot of
unnecessary task processing.
 

\section{YET TO BE DOCUMENTED}

\begin{itemize}
 \item usage
 \item how to extend with new tasks
 \item simple system monitors
 \item remote control: 
    \begin{itemize}
    \item clean shutdown of pyro
    \item bumping the dummy clock forward
    \end{itemize}
 \item external task messaging interface script
 \item config file, and optional initialisation from state dump
 \item dummying out specific tasks in real mode
\end{itemize}


\section{Miscellaneous Notes}

(To be incorporated into the main documentation, or deleted).

\subsection{catchup mode}

Note that ``correct model sequencing'' is not equivalent to ``orderly
generation of products by reference time'', in catchup operation.  E.g.
nzlam can run continuously regardless of the downstream processing that
depends on it.

Catchup vs up-to-date operation is now task-dependent, rather than a
property of the whole system (as it is if only tasks from the same
reference time can run at once).  Where this matters, it can be detected
by the relevant external task. E.g. if the external topnet(T) task
starts up at real time t greater than the streamflow data time for T
(i.e. T+15 min), i.e. the required streamflow data is already available,
then we're still in catchup. If, on the other hand, the topnet task
finds that it has to wait for its streamflow data time to arrive, then
we're caught up to real time.  This matters because topnet is allowed to
run ahead by a different amount of time depending on whether we're in
catchup mode or not.


\subsection{controlling when a task executes}

\begin{itemize}
 \item  prerequisites
 \item artificial prerequisites (e.g. make nztide depend on nzlam)
 \item delayed instantiation (a task can't run if it doesn't exist yet).
 \item other contraints based on, for example, the number of previous instances
       that still exist in the system.
\end{itemize}


\subsection{requisites}

{\em EXACT PREREQUISITES} (most tasks): times are specified exactly,
relative to the task's own reference time.  E.g. ``file foo\_{T}.nc
ready'' where T is the task's reference time.

{\em FUZZY PREREQUISITES} (topnet): a time boundary, Tb, is specified
relative to the task's own reference time, and any task with a reference
time greater than or equal to Tb can satisify the prerequisite.


\end{document}
