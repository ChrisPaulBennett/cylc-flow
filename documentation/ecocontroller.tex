\documentclass[11pt,a4paper]{report}

\usepackage{listings}
\usepackage{amsmath}

\lstset{language=Python}

\title{Multiflight Dynamic Sequencing Controller Documentation}

\author{Hilary Oliver, NIWA}

\begin{document}

\maketitle
\tableofcontents

\chapter{Introduction} 

From a control system perspective, the EcoConnect forecasting engine
consists of many distinct tasks\footnote{A {\em task} is a set of
processes that we want separate scheduling control over, as a group.}
that have to execute in an order largely determined by availability of
input files, many of which are generated by other tasks within the
system. This ordering is not a simple linear sequence; rather, there are
multiple parallel streams of execution that can branch (when one task
supplies input to several others) and merge (when one task takes input
from several others). The control system needs to be flexible (so that
tasks can be turned on and off easily) and extensible (so that new tasks
can be added easily) and it must allow tasks from different forecast
cycles to run at the same time, where dependencies allow, for maximum
throughput after system delays ({\em multiflight} operation). 

\chapter{Design Options}

The following sections describe the task sequencing method used by our
original single-flight control system, and several ideas put forward
for its multiflight successor.

\section{Explicit Sequencing Logic}

The obvious thing to try first (which indeed we did) is a Finite State
Machine that enforces the correct predetermined, hardwired, task
sequence: {\em IF file X has been generated by task A, AND task B is
finished, THEN run task C}, and so on. For a relatively simple
single-flight system this is easy to understand and it works well, but
attempting to build in flexibility or to extend the system inevitably
results in increasingly convoluted logic, and it is hard to see how to
generalise it to multiflight operation without ending up in a real mess
(note that we can't just fire off a sequence of overlapping independent
single-flight controllers because of intercycle
dependencies\footnote{The simplest of these is that $foo(T\negmedspace +\negmedspace 1)$ usually
depends on $foo(T)$ because most forecast models start from a
``background state'' that is generated by the previous forecast.}). 


\section{Dependent Scheduling}

The latest batch queue job schedulers allow users to specify rudimentary
dependencies between jobs (i.e don't start Y until X is finished, or
similar). However, this is nowhere near sophisticated enough for our
needs. 


\section{Existing Control Frameworks}

It is possible that some existing ``control framework'' could be
shoehorned into service, although I'm not aware of anything that is
obviously well suited. In any case, it is my opinion that the {\em
dynamic sequencing} design described in the next section simplifies the
problem to such an extent that any putative generic framework would find
it impossible to compete. 


\section{Dynamic Sequencing}

This method was inspired by object oriented game programming techniques.
Tasks are represented in the controller by {\em task objects} that only
care about their own states but are able to interact with others to find
out if their dependencies are satisfied yet. Thus, rather than enforcing
a prescribed task sequence from the outset, correct sequencing emerges
naturally at run time.  The total lack of explicit sequencing logic
makes this extremely flexible: new tasks can be added, or existing ones
switched on and off, without modifying the main program at all.


\chapter{Dynamic Sequencing}

\section{Task Dependencies}

Tasks have {\em prerequisites} that must be satisfied before they can
run, e.g. ``file foo is ready'' [to use], and {\em postrequisites} that
are satisfied as they run, e.g. ``file bar is ready'' [to be used by
others]. This is just the minimal external interface information needed
to run a task in isolation (i.e. what inputs are required, and what
outputs are generated) {\em but} because one task's output is another's
input, it entirely determines the global task sequence too. Explicit
sequencing logic is not needed because sequencing information is already
present, implicitly, in the matching-up of task pre- and
post-requisites. 

\section{Task Objects}

A task object knows its own prerequisites and postrequisites and can
interact with other tasks to find out if its prerequisites have been
satisfied yet. When all its prerequisites are satisfied, it can launch
its external task, after which its internal state, including notice of
completed postrequisites, is updated by remote method calls\footnote{The
Python Remote Object protocol (Pyro) allows direct network communication
between external tasks and their parent task objects in the controller}
coming in from the external task. 

Note that a task object does not need to know {\em who} is supposed to
satisfy its prerequisites because it can ask, indiscriminately, every
other task in the system. This makes the task interaction code almost
trivial, and the fact that most task interactions are fruitless is of no
consequence. 

Note also that postrequisites are just {\em messages}. If the controller
receives a ``file X ready'' message, for example, it trusts the message
source and does not bother to check for the actual file's existence. To
do otherwise would be redundant because the external tasks have to
verify the existence of input files anyway, and they can report back any
errors.

\chapter{Implementation}

To implement dynamic sequencing we need to decide how to manage the task
objects (e.g. when they will be created and destroyed). There is almost
too much flexibility here (e.g.  should tasks be created all at once, in
cotemporal batches, or one at a time as their predecessors finish?) and
consequences have to be evaluated carefully, which is not easy because
of the inherent complexity of multiflight operation. In principle we
could, for example, create a whole month's worth of tasks at once and
let them interact until everything runs to completion. That seems
refreshingly simple, and nothing would run out of sequence, {\em but}
system monitoring would be difficult because of the sheer number of
tasks involved, the end of the month presents an artificial barrier to
multiflight operation, tasks that lack prerequisites would all want to
run at once, and system restarts could be very complicated. 

The algorithm actually implemented in the controller, below, operates on
a single pool of interacting task objects, has simple start up and
continuous operation, and is relatively easy to monitor (task objects
exist by the time they are needed but not for too long before that, and
not for too long after they are spent): 

\section{Sequencing Algorithm}

The following short code listing shows the high level sequencing
algorithm taken directly from the code:

\noindent
\rule{5cm}{.2mm}
\begin{lstlisting}
while True: # MAIN LOOP
   if task_base.state_changed and not master.system_pause:
       # PROCESS ALL TASKS whenever one changes state as
       # a result of a remote task message coming in.
       task_pool.regenerate()
       task_pool.interact()
       task_pool.run_if_ready()
       task_pool.dump_state()
       if task_pool.all_finished():
           clean_shutdown( 'ALL TASKS FINISHED' )
       task_pool.kill_spent_tasks()
       task_pool.kill_lame_tasks()

    # PYRO REQUEST HANDLING; returns after one or more remote
    # method invocations is processed (not just task messages, 
    # hence the use of task_base.state_changed above).
    task_base.state_changed = False
    pyro_daemon.handleRequests( timeout = None )
# MAIN LOOP END
\end{lstlisting}


\section{Startup}

The user supplies an initial reference time and list of task names.
Create named task objects at the initial reference time {\em or} at the
first subsequent reference time that is valid for the task type.
Subsequently, new tasks are created only by {\em abdication} (below).
    \begin{itemize}
        \item tasks are uniquely identified by type and reference time. 
        \item new task objects are created in a {\em waiting} state, by
        default.
        \item we can also start from a previous {\em state dump} file
        (see below).
    \end{itemize}


An initial run through the {\em task processing block} (see below)
causes any tasks with no prerequisites (e.g. {\em downloader}) to enter the
{\em running} state and launch their external tasks immediately.
    \begin{itemize}
    \item if there are no prerequisiteless tasks, nothing will happen.
    \end{itemize}


\section{Pyro Request Handling}

The program now enters the {\em Pyro request loop}, which:

    \begin{itemize}
    \item handles remote method calls coming in from external tasks, 
    \item returns after at least one remote method call is handled. 
    \end{itemize}

Pyro must be configure to run in single-threaded mode (see Appendix
\ref{pyro-appendix}).

\section{Task Processing Block} 

The following processing is done whenever the Pyro request loop returns
and one or more tasks have changed state: 

\subsection{regenerate tasks}

    create new tasks by abdication, i.e.
    create $foo(T\negmedspace +\negmedspace 1)$ when $foo(T)$ achieves a {\em finished} state.
    \begin{itemize}
    \item this ensures that $foo(T\negmedspace +\negmedspace 1)$ won't run before $foo(T)$
    finishes, without imposing explicit intercycle prerequisites
    that would require special treatment at startup (when there is no
    previous cycle). 
    \item it also ensures that tasks with no prerequisites, e.g.
    {\em downloader} and {\em nztide}, won't all try to run at once.
    \item tasks are not deleted immediately on abdication (see below). 
    \end{itemize}


\subsection{task interaction} 

Satisfy each others' prerequisites. 

\subsection{run if ready} 

Each task object can launch its external task and enter the
    {\em running} state if:
        \begin{itemize}
        \item your prerequisites are all satisfied
        \item any existing older tasks of your type are {\em finished} 
        \item fewer than {\em MAX\_ RUNAHEAD} finished tasks of your
        type still exist (this stops tasks with no prerequisites from
        running ahead indefinitely).
        \end{itemize}

\subsection{dump state} 

The current state (waiting, running, or
    finished) of all tasks is written out to the {\em state dump file}.
        \begin{itemize}
        \item this provides a snapshot of the system just prior to shutdown.

        \item the controller can be initialised from the state dump file
            \begin{itemize}
            \item the state dump file can be edited before reloading

            \item any 'running' tasks are reloaded in the 'waiting' state.
            \end{itemize}
        \end{itemize}

    \subsection{kill spent tasks} 

A task is spent if it (a) finished,
    and (b) no longer needed to satisfy the prequisites of any other
    task.
       \begin{itemize}

       \item each non-finished task reports its {\em cutoff reference
       time}, i.e. the oldest reference time that is still needed for to
       satisfy its own prerequisites or those of its immediate successor
       after abdication.  In most cases this is just the task's own
       reference time. For hourly {\em topnet} it is the reference time
       of the previous finished 06 or 18Z {\em nzlam\_post} task (the
       next topnet task may need the same nzlam\_post).  

       \item the task manager then kills any batch of cotemporal tasks
       that are all finished {\em and} older than the oldest task cutoff
       time.

       \end{itemize}

\subsection{kill lame tasks} 

Remove tasks that will never run
    because their prerequisites cannot be satisfied by any other task in
    the system.  

       \begin{itemize}
       \item these need to be removed or they'll prevent the spent
       task deletion algorithm from working.
       
       \item they can only be detected in the {\em oldest} batch
       of cotemporal tasks. At other times more tasks may appear later
       on as their predecessors abdicate.

       \item the presence of lame tasks may indicate user error: e.g.
       forgetting to include task type $foo$ that supplies input to
       task type $bar$ will turn any instance of $bar$ into a lame
       task.

       \item lame tasks are to be expected sometimes at start up. E.g.
       if the system is started at 12Z with topnet turned on, all topnet
       tasks from 12Z through 17Z will be valid but lame (because
       they will want to take input from a non-existent nzlam\_post
       from 06Z prior to startup).

       \item lame tasks are abdicated rather than just deleted, because
       their descendents might not be lame (see topnet case above). 
       \end{itemize}


\chapter{Dummy Mode}

Dummy mode allows complete testing of the control system without running
any real external tasks\footnote{The only difference between dummy mode
and real operation, as far as the controller is concerned, is that
external dummy tasks are not delayed by resource contention.}. When it
is ready to run, a task object will launch an external dummy program
that (i) gets a list of postrequisites from the parent task object and
then (ii) reports back that each one is satisfied at the (estimated)
right time relative to an accelerated dummy clock. Dummy tasks therefore
complete in approximately the same dummy clock time as the real tasks do
in real time. An initial dummy clock offset relative to the initial
reference time can also be specified, which allows simulation of the
transition between catchup and real time operation. Log messages are
stamped with dummy clock time instead of real time.

The same script is used for all external dummy tasks but it has special
behaviour in certain cases: the dummy downloader ``waits for incoming
files'' until 3:15 past its reference time, and the dummy topnet ``waits
for streamflow data'' until 0:15 past its reference time.

The dummy clock can be bumped forward a number of hours by remote
control, while the system is running. This affects the postrequisite
timing of running tasks correctly, but if it causes a running task to
finish immediately the next task in line will still start from the
beginning no matter how big the bump.


\chapter{Program Usage}

All user input is configured in the file \verb#config.py#. There is one
commandline option to force a restart from the state dump file (which
may have been edited):

\lstset{language=sh}

\noindent
\rule{5cm}{.2mm}
\begin{lstlisting}
ecocontroller [-r]
Options:
    + most inputs should be configured in config.py
    + [-r] restart from the state dump file
\end{lstlisting}

\lstset{language=Python}

\section{Controller Config File}

\noindent
\rule{5cm}{.2mm}
\lstinputlisting{../config.py}

\appendix

\chapter{Object Oriented Programming}

This is a minimal introduction to OOP concepts that the controller is
critically dependent on. Refer to any OOP reference for more detail.

\section{Classes and Objects}

A {\em class definition} declares the properties of {\em objects} or
{\em instances} of that class; objects are self-contained entities that
can be assigned to a variable and have specific {\em state} (data) and
{\em behaviour} (functions or ``methods'' that operate on the data and
are the means by which objects interact with the outside world).    

For example, a $shape$ class could define a $position$ data member that
describes the location of each shape object, a $move()$ method that
causes a shape object to alter its position, and a $draw()$ method that
causes it to display itself in the right place on screen.

\section{Inheritance}

A {\em derived class} or {\em subclass} inherits the properties (methods
and data members) of its {\em parent class}. It can also {\em override}
specific parent class properties, or add new properties that aren't
present in the parent class. Calling a particular method on an object
invokes the object's own class method if one is defined, otherwise the
parent class is searched, and so on down to the {\em base class} at the
root of the object's inheritance tree. 

For example, we could derive a $circle$ class from $shape$, adding a
`radius' data member and overriding the $draw()$ to get circle objects
to display themselves as actual circles.  Because we didn't override the
$move()$ method, calling $circle.move()$ would invoke the base class
method, $shape.move()$. 


\section{Polymorphism}

This refers to the ability to treat a group of objects as if they were
all members of a common base class.

For example, we could construct a list of $shape$ objects from
$circles$, $triangles$, and $squares$; calling $[list member].draw()$
will invoke the right derived class $draw()$. This is a very powerful
mechanism because {\em it allows unmodified old code to call new code}:
if we later derive an entirely new kind of shape ($hexagon$, say) with
it's own unique behaviour, the existing program, without modification,
will process the new objects in the proper hexagon-specific way.

\section{The OO Controller}

Task objects are polymorphic: tasks of all types are stored and
manipulated in a single list as if they were instances of the base class
that defines common task behaviour, but their actual behaviour is
derived-class-specific when necessary (calling $nztide.run()$ results in
the tide model running, and so on). 


\chapter{Threading in Pyro} \label{pyro-appendix}

In SINGLE THREADED PYRO, handleRequests() returns after EITHER a timeout has
occurred OR at least one request (remote method call) was handled.  With
``timeout = None'' this allows us to process tasks ONLY after remote method
invocations come in. Further, the processing\_required boolean set in
task\_base.incoming() allows us to process tasks ONLY when a task changes state
as a result of an incoming message, which minimizes non-useful output from the
task processing loop (e.g. in dummy mode there are a lot of remote calls on
the dummy clock object, which does not alter tasks at all). 

In MULTITHREADED PYRO, handleRequests() returns immediately after creating a
new request handling thread for a single remote object and thereafter remote
method calls on that object come in asynchronously in the dedicated thread.
It is impossible(?) to make our main loop work properly like this because
handleRequests will block until a new connection is made, even while messages
from existing remote objects are coming in.  Tasks that are ready to run are
only set running in the processing loop, so these will be delayed
unnecessarily until handleRequests returns.  The only way out of this is to do
task processing on a handleRequests timeout as well, which results in a lot of
unnecessary task processing.
 


\chapter{YET TO BE DOCUMENTED}

\begin{itemize}
 \item usage
 \item logging
 \item debugging
 \item adding new task definitions
 \item simple system monitors
 \item remote control: 
    \begin{itemize}
    \item clean shutdown of pyro
    \item bumping the dummy clock forward
    \end{itemize}
 \item external task messaging interface script
 \item dummying out tasks in real mode
\end{itemize}

\chapter{Miscellaneous Notes}

(To be incorporated into the main documentation, or deleted).

\section{catchup mode}

Note that ``correct model sequencing'' is not equivalent to ``orderly
generation of products by reference time'', in catchup operation.  E.g.
nzlam can run continuously regardless of the downstream processing that
depends on it.

Catchup vs up-to-date operation is now task-dependent, rather than a
property of the whole system (as it is if only tasks from the same
reference time can run at once).  Where this matters, it can be detected
by the relevant external task. E.g. if the external topnet(T) task
starts up at real time t greater than the streamflow data time for T
(i.e. $T+15$ min), i.e. the required streamflow data is already available,
then we're still in catchup. If, on the other hand, the topnet task
finds that it has to wait for its streamflow data time to arrive, then
we're caught up to real time.  This matters because topnet is allowed to
run ahead by a different amount of time depending on whether we're in
catchup mode or not.


\section{controlling when a task executes}

\begin{itemize}
 \item  prerequisites
 \item artificial prerequisites (e.g. make nztide depend on nzlam)
 \item delayed instantiation (a task can't run if it doesn't exist yet).
 \item other contraints based on, for example, the number of previous
 instances that still exist in the system.
\end{itemize}


\section{requisites}

{\em EXACT PREREQUISITES} (most tasks): times are specified exactly,
relative to the task's own reference time.  E.g. ``file foo\_{T}.nc
ready'' where T is the task's reference time.

{\em FUZZY PREREQUISITES} (topnet): a time boundary, Tb, is specified
relative to the task's own reference time, and any task with a reference
time greater than or equal to Tb can satisify the prerequisite.

\end{document}
