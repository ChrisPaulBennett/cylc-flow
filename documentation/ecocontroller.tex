\documentclass[11pt,a4paper]{report}

\usepackage{listings}
\usepackage{amsmath}

\lstset{language=Python}

\title{Multiflight Dynamic Sequencing Controller Documentation}

\author{Hilary Oliver, NIWA}

\begin{document}

\maketitle
\tableofcontents

\chapter{Task Sequencing} 

From a control system perspective, the EcoConnect forecasting engine
consists of many distinct tasks\footnote{A {\em task} is a set of
processes that we want separate scheduling control over, as a group.}
that have to execute in an order largely determined by availability of
input files, many of which are generated by other tasks within the
system. This ordering is not a simple linear sequence; rather, there are
multiple parallel streams of execution that can branch (when one task
supplies input to several others) and merge (when one task takes input
from several others). The control system needs to be flexible (so that
tasks can be turned on and off easily) and extensible (so that new tasks
can be added easily), it has to control jobs on several machines at
once, and it must allow tasks from different forecast cycles to run at
the same time, where dependencies allow, for maximum throughput after
system delays ({\em multiflight} operation). 


\chapter{The Original Controller}

The original single-flight controller, which ran the whole system pretty
reliably for two or three years, is a Finite State Machine that enforces
the predetermined hardwired task sequence using explicit sequencing
logic: {\em IF file X has been generated by task A, AND task B is
finished, THEN run task C}, and so on. The program's event loop handles
new messages logged to \verb#/var/log/ecoconnect# by the external tasks.

\section{Deficiencies}

This method is easy to understand, and it works well for a relatively
simple single flight system.  Attempting to build in flexibility or to
extend the system, however, inevitably results in increasingly
convoluted logic, and it is hard to see how to generalise the method to
multiflight operation without ending up in a real mess (note that we
can't just fire off a sequence of overlapping independent single-flight
controllers because of intercycle dependencies\footnote{The simplest of
these is that $foo(T\negmedspace +\negmedspace 1)$ usually depends on
$foo(T)$ because most forecast models start from a ``background state''
that is generated by the previous forecast.}). 

\section{Replacement}

We considered basing the new multiflight controller on a generic control
framework of some kind, but nothing that was obviously well suited came
to light. In any case the {\em dynamic sequencing} design described in
the next chapter simplifies the main problem to such an extent that, in
my view, it is unlikely that any generic framework could compete. I note
also that while some batch queue schedulers now allow simple job
dependencies ({\em don't run Y until X is finished}) they are still far
too rudimentary for our purposes.


\chapter{New Controller Implementation}

\section{Dynamic Sequencing Overview}

The dynamic sequencing method was inspired by object oriented game
programming techniques. Tasks are represented in the controller by
self-contained {\em task objects} that are able to interact to find out
if their dependencies are satisfied yet. Thus, rather than enforcing a
prescribed task sequence from the outset, correct sequencing emerges
naturally at run time. The total lack of explicit sequencing logic makes
this approach extremely flexible: new tasks can be added, or existing
ones switched on and off, without any change to the main program.

\subsection{Task Requisites}

Tasks have {\em prerequisites} that must be satisfied before they can
run, e.g. ``file foo is ready'' [to use], and {\em postrequisites} that
are satisfied as they run, e.g. ``file bar is ready'' [to be used by
others]. This is just the minimal external interface information needed
to run a task in isolation (i.e. what inputs are required, and what
outputs are generated) {\em but} because one task's output is another's
input, it entirely determines the global task sequence too. Explicit
sequencing logic is not needed because sequencing information is already
present, implicitly, in the matching of pre- and post-requisites. 

Note that ``requisites'' are just {\em messages} that indicate a task
has reached a particular waypoint.  If the controller receives a ``file
X ready'' message, for example, it trusts the message source and does
not bother to check that the actual file exists. To do so would be
redundant because all external tasks have to verify the existence of
their input files anyway, and they can report back any errors.


\subsection{Task Objects}

The task objects that represent our real tasks need to know their own
prerequisites and postrequisites, and to be able to interact with other
tasks to find out if their prerequisites are satisfied yet. When all of
a task's prerequisites are satisfied, it can launch its associated
external task. Once running, the object's state (e.g. knowledge of which
postrequisites are completed so far) must be updated by remote method
calls from the external task\footnote{The Python Remote Object protocol
(Pyro) allows direct network communication between external tasks and
their parent task objects in the controller}. 

Task objects do not need to know {\em who} is supposed to satisfy their
prerequisites because they can ask, indiscriminately, every other task
in the system; this makes the task interaction code almost trivial. The
fact that most of these task interactions are fruitless is of no
consequence. 

\subsection{Task Management}

To implement dynamic sequencing we need a specific task management
scheme that controls when tasks will be created and destroyed, and so
on. There is almost too much flexibility here (e.g.  should tasks be
created all at once, in cotemporal batches, or one at a time as their
predecessors finish?) and consequences have to be evaluated carefully,
which is not easy because of the inherent complexity of multiflight
operation. In principle we could, for example, create a whole month's
worth of tasks at once and let them interact until everything runs to
completion. That seems refreshingly simple, and nothing would run out of
sequence, {\em but} system monitoring would be difficult because of the
sheer number of tasks involved, the end of the month presents an
artificial barrier to multiflight operation, tasks that lack
prerequisites would all want to run at once, and system restarts could
be very complicated. 


\section{The Algorithm}

The algorithm actually implemented in the controller, below, operates on
a single pool of interacting task objects, has simple start up and
continuous operation, and is relatively easy to monitor (task objects
exist by the time they are needed but not for too long before that, and
not for too long after they are finished). 

The simplicity of the dynamic sequencing algorithm is clear from the
following code listing, taken directly from the main program:

\pagebreak

\noindent
\rule{5cm}{.2mm}
\begin{lstlisting}
# (startup initialization code omitted)

while True: # MAIN LOOP

   if task_base.state_changed:
       # PROCESS ALL TASKS whenever one changes state as
       # a result of a remote task message coming in.

       task_pool.regenerate()  # create new tasks
       task_pool.interact()
       task_pool.run_if_ready()
       task_pool.dump_state()
       if task_pool.all_finished():
           clean_shutdown( "ALL TASKS FINISHED" )
       task_pool.kill_spent_tasks()
       task_pool.kill_lame_tasks()

    # PYRO REQUEST HANDLING; returns after one or more remote
    # method invocations are processed (these are not just task
    # messages, hence the use of task_base.state_changed above).

    task_base.state_changed = False
    pyro_daemon.handleRequests( timeout = None )

# MAIN LOOP END
\end{lstlisting}


\subsection{Startup and Initialization}

An initial reference time and list of task object names are read in from
the config file, then each task object is created at the initial
reference time {\em or} at the first subsequent reference time that is
valid for the task type. Optionally, we can tell the controller to
reload the current state dump file (which may have been edited); this
will override the configured start time and task list. After startup,
new tasks are created only by {\em abdication} (below).

An initial run through the {\em task processing} code, by virtue of the
fact that the main loop starts with task processing, causes tasks with
no prerequisites (e.g. {\em downloader}) to enter the {\em running}
state and launch their external tasks immediately. Otherwise ({\em or}
if there are no tasks that lack prerequisites) nothing will happen.


\subsection{Remote Method Calls (Pyro)}

The Pyro request handling loop processes remote method calls coming in
from external tasks, and returns after at least one remote method call
was handled. Pyro must be run in non-default single-threaded mode (see
Appendix \ref{pyro-appendix}).


\subsection{Task Processing}

The following operations are performed whenever one or more task objects
have changed state through the action of remote method calls: 

\subsubsection{Task Regeneration}

New tasks are created by abdication, i.e. create $foo(T\negmedspace
+\negmedspace 1)$ if $foo(T)$ is {\em finished}.  Task abdication
ensures that $foo(T\negmedspace +\negmedspace 1)$ won't run before
$foo(T)$ finishes, without imposing explicit intercycle prerequisites
that would require special treatment at startup (when there is no
previous cycle).  It also ensures that tasks with no prerequisites, e.g.
{\em downloader} and {\em nztide}, won't all try to run at once.
Tasks are not deleted immediately on abdication (see below).


\subsubsection{Task Interaction} 

Each task keeps track of which of its postrequisites are completed, and
asks the other tasks if they can satisfy any of its prerequisites. 

\subsubsection{Running Ready Tasks}

Each task object can launch its associated external task, and enter the
{\em running} state if its prerequisites are all satisfied, any existing
older tasks of the same type are already {\em finished}, and fewer than
{\em MAX\_ RUNAHEAD} finished tasks of the same type still exist (this
stops tasks with no prerequisites from running ahead indefinitely).

\subsubsection{Dump State} 

The current state (waiting, running, or finished) of each task is
written out to the {\em state dump file}.  This provides a running
snapshot of the system. Most importantly, it records the state of the
system just prior to shutdown. The controller can optionally start up
by loading the state dump (which can be edited first). Any 'running'
tasks are reloaded in the 'waiting' state.

\subsubsection{Removing Spent Tasks} 

A task is spent if it finished {\em and} no longer needed to satisfy the
prequisites of any other task. Each non-finished task reports its {\em
cutoff reference time}, i.e. the oldest reference time that may contain
tasks still needed to satisfy its own prerequisites or those of its
immediate post-abdication successor.  In most cases this is just the
task's own reference time. For hourly {\em topnet} it is the reference
time of the previous finished 06 or 18Z {\em nzlam\_post} task (the next
topnet task may need the same nzlam\_post). The task manager can then
kill any batch of cotemporal tasks that are all finished {\em and} older
than the oldest task cutoff time.

\subsubsection{Removing Lame Tasks} 

Tasks that will never run (because their prerequisites cannot be
satisfied by any other task in the system) are removed from the {\em
oldest batch} of tasks.  These need to be removed or they'll prevent the
spent task deletion algorithm from working. Lame tasks can only be
detected in the oldest task batch; in later batches new tasks may still
appear as their predecessors abdicate.

Lame tasks are abdicated rather than just deleted, because their
descendents will not necessarily be lame: e.g. if the system is started
at 12Z with topnet turned on, all topnet tasks from 12Z through 17Z will
be valid but lame, because they will want to take input from a
non-existent nzlam\_post from 06Z prior to startup. However, the
presence of lame tasks may indicate user error: e.g. if you forget
to turn on task type $foo$ that supplies input to task type $bar$,
any instance of $bar$ will be lame.


\chapter{Dummy Mode}

Dummy mode allows complete testing of the control system without running
any real external tasks\footnote{The only difference between dummy mode
and real operation, as far as the controller is concerned, is that
external dummy tasks are not delayed by resource contention.}. When it
is ready to run, a task object will launch an external dummy program
that (i) gets a list of postrequisites from the parent task object and
then (ii) reports back that each one is satisfied at the (estimated)
right time relative to an accelerated dummy clock. Dummy tasks therefore
complete in approximately the same dummy clock time as the real tasks do
in real time. An initial dummy clock offset relative to the initial
reference time can also be specified, which allows simulation of the
transition between catchup and real time operation. Log messages are
stamped with dummy clock time instead of real time.

The same script is used for all external dummy tasks but it has special
behaviour in certain cases: the dummy downloader ``waits for incoming
files'' until 3:15 past its reference time, and the dummy topnet ``waits
for streamflow data'' until 0:15 past its reference time.

The dummy clock can be bumped forward a number of hours by remote
control, while the system is running. This affects the postrequisite
timing of running tasks correctly, but if it causes a running task to
finish immediately the next task in line will still start from the
beginning no matter how big the bump.


\chapter{Program Usage}

All user-configurable parameters are set in \verb#config.py#. There is
one commandline option to force a restart from the state dump file
(which may have been edited) instead of the configured start time and
task list:

\lstset{language=sh}

\noindent
\rule{5cm}{.2mm}
\begin{lstlisting}
ecocontroller [-r]
Options:
    + most inputs should be configured in config.py
    + [-r] restart from the state dump file
\end{lstlisting}

\lstset{language=Python}

\section{Config File}

\noindent
\rule{5cm}{.2mm}
\lstinputlisting{../config.py}

\appendix

\chapter{Essential OOP Concepts}

Task objects in the new controller are {\em polymorphic}: tasks of all
types are stored and manipulated in a single list as if they were
instances of the base class that defines common task behaviour, but
their actual behaviour is derived-class-specific when necessary (for
example, calling $nztide.run()$ results in the tide model running, and
so on). The section is a minimal introduction to the Object Oriented
Program concepts needed to understand what this means.  Refer to any OOP
reference for more detail.

\section{Classes and Objects}

A {\em class definition} declares the properties of {\em objects} or
{\em instances} of that class; objects are self-contained entities that
can be assigned to a variable and have specific {\em state} (data) and
{\em behaviour} (functions or ``methods'' that operate on the data and
are the means by which objects interact with the outside world).    

For example, a $shape$ class could define a $position$ data member that
describes the location of each shape object, a $move()$ method that
causes a shape object to alter its position, and a $draw()$ method that
causes it to display itself in the right place on screen.

\section{Inheritance}

A {\em derived class} or {\em subclass} inherits the properties (methods
and data members) of its {\em parent class}. It can also {\em override}
specific parent class properties, or add new properties that aren't
present in the parent class. Calling a particular method on an object
invokes the object's own class method if one is defined, otherwise the
parent class is searched, and so on down to the {\em base class} at the
root of the object's inheritance tree. 

For example, we could derive a $circle$ class from $shape$, adding a
`radius' data member and overriding the $draw()$ to get circle objects
to display themselves as actual circles.  Because we didn't override the
$move()$ method, calling $circle.move()$ would invoke the base class
method, $shape.move()$. 


\section{Polymorphism}

This refers to the ability to treat a group of objects as if they were
all members of a common base class.  For example, we could construct a
list of $shape$ objects from $circles$, $triangles$, and $squares$;
calling $[list member].draw()$ will invoke the right derived class
$draw()$. This is a very powerful mechanism because {\em it allows
unmodified old code to call new code}: if we later derive an entirely
new kind of shape ($hexagon$, say) with it's own unique behaviour, the
existing program, without modification, will process the new objects in
the proper hexagon-specific way.


\chapter{Threading in Pyro} \label{pyro-appendix}

With Pyro in {\em single threaded mode}, \verb#handleRequests()# returns
after {\em either} a timeout has occurred {\em or} at least one request
(i.e.  remote method call) was handled. With \verb#timeout = None# this
allows us to process tasks {\em only} after remote method invocations
come in.  Further, we can detect the remote calls that actually change
task states, and thereby drop into the task processing code only when
necessary, which minimizes non-useful output from the task processing
loop (e.g. in dummy mode there are a lot of remote calls on the dummy
clock object, which does not alter tasks at all). 

In {\em multithreaded mode}, \verb#handleRequests()# returns immediately
after creating a new request handling thread for a single remote object
and thereafter remote method calls on that object come in asynchronously
in the dedicated thread. This is not good for the dynamic sequencing
algorithm because tasks are only set running in the task processing
block which can be delayed while \verb#handleRequests()# blocks waiting
for a new connection to be established even as messages that warrent
task processing are coming in on existing connections. The only way
around this is to do task processing on \verb#handleRequests()# timeouts
which results in a lot of unnecessary task processing when nothing
important is happening.


\chapter{YET TO BE DOCUMENTED}

\begin{itemize}
 \item usage
 \item logging
 \item debugging
 \item adding new task definitions
 \item simple system monitors
 \item remote control: 
    \begin{itemize}
    \item clean shutdown of pyro
    \item bumping the dummy clock forward
    \end{itemize}
 \item external task messaging interface script
 \item dummying out tasks in real mode
\end{itemize}

\chapter{Miscellaneous Notes}

(To be incorporated into the main documentation, or deleted).

\section{catchup mode}

Note that ``correct model sequencing'' is not equivalent to ``orderly
generation of products by reference time''.  Nzlam can run continuously,
regardless of the downstream processing that depends on it.

Catchup mode is now task-dependent, rather than a property of the whole
system (as it is if only tasks from the same reference time can run at
once).  Where this matters, it can be detected by the relevant external
task. E.g. if the external topnet(T) task starts up at real time t
greater than the streamflow data time for T (i.e. $T+15$ min), i.e. the
required streamflow data is already available, then we're still in
catchup. If, on the other hand, the topnet task finds that it has to
wait for its streamflow data time to arrive, then we're caught up to
real time.  This matters because topnet is allowed to run ahead by a
different amount of time depending on whether we're in catchup mode or
not.


\section{controlling when a task executes}

\begin{itemize}
 \item  prerequisites
 \item artificial prerequisites (e.g. make nztide depend on nzlam)
 \item delayed instantiation (a task can't run if it doesn't exist yet).
 \item other contraints based on, for example, the number of previous
 instances that still exist in the system.
\end{itemize}


\section{fuzzy prequisites}

{\em Exact prerequisites} (most tasks): times are specified exactly,
relative to the task's own reference time.  E.g. {\em file foo\_{T}.nc
ready} where T is the task's reference time.

{\em Fuzzy prerequisites} (topnet): a time boundary is specified
relative to the task's own reference time; any task with a reference
time greater than or equal to the boundary time can satisify the
prerequisite.

\end{document}
