#!/usr/bin/env python

#         __________________________
#         |____C_O_P_Y_R_I_G_H_T___|
#         |                        |
#         |  (c) NIWA, 2008-2010   |
#         | Contact: Hilary Oliver |
#         |  h.oliver@niwa.co.nz   |
#         |    +64-4-386 0461      |
#         |________________________|

import os, sys, re
import socket, pickle
import logging, logging.handlers
from optparse import OptionParser
from cylc_pyro_server import pyro_base_port, pyro_port_range
import Pyro.core, Pyro.errors, Pyro.naming
from Pyro.ext.daemonizer import Daemonizer
from ConfigParser import SafeConfigParser
from suite_id import identifier

import os, sys, pwd, re
import logging

class ServerDaemon( Daemonizer ):
    def __init__( self, pid_file, log_file, state_file ):
        self.log_file = log_file
        self.state_file = state_file
        Daemonizer.__init__(self, pid_file)
        self.pid_file = pid_file

    def main_loop( self ):
        # NOTE THE PYRO SERVER BEING DAEMONIZED MUST BE STARTED HERE
        # OTHERWISE ITS SOCKET FILE DESCRIPTORS WILL BE CLOSED DOWN IN
        # Pyro's daemonizer.__init__() (a Pyro bug, I think).

        # for debugging in daemon mode, uncomment the following so that
        # exceptions will be written to log files.

        #foo = open( "/home/oliverh/cylc/foo.txt", "a" )
        #server = None
        #try:
        server = LServer( self.pid_file, self.log_file, self.state_file )
        #except Exception, x:
        #    foo.write( str(x) + "\n" )
        #else:
        #    pass

        #bar = open( "/home/oliverh/cylc/bar.txt", "a" )
        #try:
        server.start()
        #except Exception, x:
        #    bar.write( str(x) + "\n" )
        #else:
        #    pass

class LServer:
    def __init__( self, pid_file, log_file, state_file ):
        self.log_file = log_file
        self.pid_file = pid_file
        self.state_file = state_file

        Pyro.config.PYRO_MULTITHREADED = 0
        # USE DNS NAMES INSTEAD OF FIXED IP ADDRESSES FROM /etc/hosts
        # (see the Userguide "Networking Issues" section).
        Pyro.config.PYRO_DNS_URI = True
        # UNCOMMENT THE FOLLOWING FOR LOGGING (goes to $PWD/Pyro_log)
        #Pyro.config.PYRO_STDLOGGING = True
        #Pyro.config.PYRO_TRACELEVEL = 3

        # base (lowest allowed) Pyro socket number
        Pyro.config.PYRO_PORT = pyro_base_port
        # max number of sockets starting at base
        Pyro.config.PYRO_PORT_RANGE = pyro_port_range

        Pyro.core.initServer()

    def start( self ):
        # CREATE A PYRO pyro_daemon FOR THIS SUITE
        # (single threaded prob not necessary for lockserver
        # but it doesn't matter as few connections needed)

        self.owner = os.environ[ 'USER' ]
        self.pyro_daemon = Pyro.core.Daemon()
        port = self.pyro_daemon.port
        print "Listening on port", port
        locker = lockserver( self.owner, port, self.pid_file, self.log_file, self.state_file )


        lockserver_id = identifier( 'lockserver', self.owner )
        self.pyro_daemon.connect( lockserver_id, 'cylcid' )

        #foo = open( "/home/oliverh/cylc/foo.txt", "a" )

        qualified_name = self.owner + '.lockserver'
        self.pyro_daemon.connect( locker, qualified_name )


        while True:
            self.pyro_daemon.handleRequests()

class lockserver( Pyro.core.ObjBase ):
    def __init__( self, owner, port, pid_file, log_file, state_file, loglevel=logging.INFO ):
        Pyro.core.ObjBase.__init__(self)

        # pid file is passed through to here purely so it can be
        # retrieved by user request, through 'cylc lockserver -f'.
        self.owner = owner
        self.port = port
        self.pid_file = pid_file
        self.log_file = log_file
        self.state_file = state_file

        self.configure_logging( log_file, loglevel )

        if self.load_state():
            self.log.info( 'Loaded initial state from ' + self.log_file )
        else:
            self.log.info( 'Starting with a clean slate' )
            # task locks
            self.locked = {}
            # suite locks
            self.exclusive = {}       # exclusive[ suite_dir ] = [ groupname ]
            self.inclusive = {}       # inclusive[ suite_dir ] = [ groupname, ... ]

    #def identify( self ):
    #    return 'cylc lockserver', self.owner 

    def dump_state( self ):
        output = open( self.state_file, 'w' )
        pickle.dump( ( self.exclusive, self.inclusive, self.locked ), output )
        output.close()

    def load_state( self ):
        if not os.path.exists( self.state_file ):
            print "File not found:", self.state_file
            print "(i.e. no previous state to load)"
            return False

        state = open( self.state_file, 'rb' )
        ( self.exclusive, self.inclusive, self.locked ) = pickle.load( state )
        state.close()
        return True

    def configure_logging( self, log_file, loglevel ):
        self.log = logging.getLogger( log_file )
        self.log.setLevel( loglevel )
        max_bytes = 1000000
        backups = 5
        if os.path.basename( log_file ) != log_file:
            logging_dir = os.path.dirname( log_file )
            if not os.path.exists( logging_dir ):
                try:
                    os.makedirs( logging_dir )
                except:
                    raise SystemExit( 'Failed to create logging directory ' + logging_dir + '!')

        h = logging.handlers.RotatingFileHandler( log_file, 'a', max_bytes, backups )
        # roll the log file if it already exists
        if os.path.getsize( log_file ) > 0:
            h.doRollover()

        f = logging.Formatter( '%(asctime)s %(levelname)-2s - %(message)s', '%Y/%m/%d %H:%M:%S' )
        h.setFormatter(f)
        self.log.addHandler( h )

    def get_lock_id( self, lockgroup, task_id ):
        return lockgroup + ':' + task_id

    def get_lock_owner( self, lockgroup ):
        owner, suite = lockgroup.split( '.' )
        return owner

    def get_lockgroup( self, lock_id ):
        (lockgroup, id) = lock_id.split(':')
        return lockgroup

    def get_suite_string( self, lockgroup, suite_dir ):
        return lockgroup + '-->' + suite_dir

    def acquire( self, task_id, lockgroup ):
        id = self.get_lock_id( lockgroup, task_id )
        if id not in self.locked:
            self.locked[ id ] = True
            self.log.info( "acquired task lock " + id ) 
            self.dump_state()
            return True
        else:
            self.log.warning( "refused task lock " + id ) 
            return False

    def release( self, task_id, lockgroup ):
        id = self.get_lock_id( lockgroup, task_id )
        if id in self.locked:
            del self.locked[ id ]
            self.log.info( "released task lock " + id ) 
            self.dump_state()
            return True
        else:
            self.log.warning( "failed to release task lock " + id ) 
            return False

    def get_filenames( self ):
         self.log.info( "Returning server filenames") 
         return ( self.pid_file, self.log_file, self.state_file )

    def dump( self ):
         self.log.info( "Returning lock information") 
         return ( self.locked.keys(), self.exclusive, self.inclusive )

    def clear( self, user ):
        # release all locks one at a time so each release gets logged
        self.log.info( "Releasing ALL locks held by " + user ) 

        # MUST USE .keys() here to avoid:
        # RuntimeError: dictionary changed size during iteration

        for suitedir in self.exclusive.keys():
            [ group ] = self.exclusive[ suitedir ]
            if self.get_lock_owner( group ) == user:
                self.release_suite_access( suitedir, group )

        for suitedir in self.inclusive.keys():
            groups = self.inclusive[ suitedir ]
            for group in groups:
                print "+", group
                print " ", self.get_lock_owner( group ), user
                if self.get_lock_owner( group ) == user:
                    self.release_suite_access( suitedir, group )

        # release task locks with no associated suite lock
        # (acquired manually, or through 'cylc submit').
        for lock in self.locked.keys():
            ( group, id ) = lock.split( ':' )
            if self.get_lock_owner( group ) == user:
                self.release( id, group )

        self.dump_state()
        return

    def is_locked( self, task_id, lockgroup ):
        id = self.get_lock_id( lockgroup, task_id )
        if id in self.locked:
            return True
        else:
            return False

    def get_suite_access( self, suite_dir, lockgroup, cylc_mode, request_exclusive ):
        # EXCLUSIVE: one only named suite can use suite_dir at once
        #   - submit can attempt to get a task lock IF via the same name
        # INCLUSIVE: multiple named suites can use suite_dir at once
        #   - submit can attempt to get a task lock always

        suite_descr = self.get_suite_string( lockgroup, suite_dir ) 

        result = True
        reason = "granted"
 
        if cylc_mode != 'submit':
            if ( request_exclusive and suite_dir in self.inclusive ) or \
                    ( not request_exclusive and suite_dir in self.exclusive ):
                result = False
                reason = "inconsistent exclusivity for " + suite_dir
                self.log.warning( reason ) 
                return ( False, reason )
 
        if request_exclusive:
            if suite_dir in self.exclusive:
                name = self.exclusive[ suite_dir ][0]
                already = self.get_suite_string( name, suite_dir )

                if cylc_mode == 'submit':
                    # grant access only if lockgroup is the same
                    if lockgroup == name:
                        pass
                    else:
                        result = False
                        reason = self.get_suite_string( name, suite_dir ) + " in exclusive use"
                else:
                    # no exclusive access to any suite already in use
                    result = False
                    reason = suite_descr + " in exclusive use" 
            else:
                # suite dir not already in self.exclusive
                if cylc_mode == 'submit':
                    # grant access but don't set a lock
                    pass 
                else: 
                    # grant exclusive access
                    self.exclusive[ suite_dir ] = [ lockgroup ]
        else:
            # inclusive access requested
            if suite_dir in self.inclusive:
                names = self.inclusive[ suite_dir ]

                if cylc_mode == 'submit':
                    # granted
                    pass
                else:
                    # grant access unless same name already in use
                    if lockgroup in names:
                        result = False
                        reason =  lockgroup + '-->' + suite_dir + " already in use"
                    else:
                        # granted
                        print 'APPENDING', lockgroup
                        self.inclusive[ suite_dir ].append( lockgroup )
            else:
                if cylc_mode == 'submit':
                    # granted
                    pass
                else:
                    # granted
                    print 'STARTING', lockgroup
                    self.inclusive[ suite_dir ] = [ lockgroup ]
 
        if result:
            self.dump_state() 
            if cylc_mode == 'submit':
                self.log.info( "granted suite access " + lockgroup + " --> " + suite_dir )
            else:
                self.log.info( "acquired suite lock " + lockgroup + " --> " + suite_dir )
        else:
            if cylc_mode == 'submit':
                self.log.warning( "refused suite access " + lockgroup + " --> " + suite_dir )
            else:
                self.log.warning( "refused suite lock " + lockgroup + " --> " + suite_dir )
            self.log.warning( " " + reason )

        return ( result, reason )


    def release_suite_access( self, suite_dir, lockgroup ):
        # first release any task locks held by the suite
        for id in self.locked.keys():
            print id
            print self.get_lockgroup( id ), lockgroup
            if self.get_lockgroup( id ) == lockgroup:
                del self.locked[ id ]
                self.log.info( "released task lock " + id ) 

        result = True
        if suite_dir in self.exclusive:
            if lockgroup not in self.exclusive[ suite_dir ]:
                #self.log.warning( "suite release group name error" )
                result = False
            else:
                del self.exclusive[ suite_dir ]
                result = True
        elif suite_dir in self.inclusive:
            names = self.inclusive[ suite_dir ]
            if lockgroup not in names:
                #self.log.warning( "suite release group name error" )
                result = False
            elif len( names ) == 1:
                del self.inclusive[ suite_dir ]
                result = True
            else:
                self.inclusive[ suite_dir ].remove( lockgroup )
                result = True
        else:
            #self.log.warning( "erroneous suite release request" )
            result = False
        if result:
            self.dump_state() 
            self.log.info( "released suite lock " + lockgroup + " --> " + suite_dir )
        else:
            self.log.warning( "failed to release suite lock " + lockgroup + " --> " + suite_dir )

        return result

if __name__ == "__main__":

    usage = """Usage: cylclockd [-f CONFIG] ACTION 

The cylc lockserver daemon brokers suite and task locks for all cylc
suites on the network. The command line user interface for interrogating
the daemon, and for manual lock management, is 'cylc lockserver'.

The daemon should be started at boot time, running under a special system
user account.  At startup, it reads a config file '/etc/cylc/cylclockd.conf' that
specifies the location of the daemon's process ID, state, and log files. 
The state file records currently held locks and, if it exists at
startup, is used to initialize the lockserver (i.e. suite and task locks
are not lost if the lockserver is restarted). All locking activitiy is
recorded in the log file. 

For testing cylclockd as a normal user, use the [-f] command line option
to specify your own PID, state, and log files in your own config file.

Arguments:
  ACTION   -  'start', 'stop', 'status', 'restart', or 'debug'.
              debug starts cylclockd without 'daemonizing', i.e. it won't
              detach from the terminal and stdout and stderr are not lost."""

    #types = [ 'user', 'host' ]

    parser = OptionParser( usage )

    parser.set_defaults( config = '/etc/cylc/cylclockd.conf' )

    parser.add_option( "-c", "--config-file", 
            help="Config file (default /etc/cylc/cylclockd.conf)",
            metavar="CONFIGFILE", action="store", dest="config" )

    #parser.add_option( "-t", "--type", 
    #        help="Type of lockserver: " + ', '.join(types) + ".",
    #        default='user', metavar=TYPE, choices=types,
    #        action="store", dest="type" )

    ( options, args ) = parser.parse_args()

    if len( args ) != 1:
        parser.error( "Wrong number of arguments" )
    if args[0] not in [ 'start', 'stop', 'status', 'restart', 'debug' ]:
        parser.error(  "Illegal argument: " + args[0] )

    # load config file
    config = SafeConfigParser()
    config_file = options.config
    if os.path.exists( config_file ):
            print "Loading Config File: " + config_file
            config.read( config_file )
    else:
        raise SystemExit( "File not found: " + config_file )
 
    # extract config items
    pid_file   = config.get( 'main', 'pid file' )
    log_file   = config.get( 'main', 'log file' )
    state_file = config.get( 'main', 'state file' ) 
    #type = options.type

    print "PID File:        ", pid_file
    print "Logging To:      ", log_file
    print "Persistent State:", state_file
    #print "Lockserver Type: ", type

    server_daemon = ServerDaemon( pid_file, log_file, state_file ) 
    server_daemon.process_command_line( [ sys.argv[0] ] + args )
