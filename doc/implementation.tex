

!!!!!!!!!!MUCH OF THIS IS OBSOLETE OR NEEDS UPDATING!!!!!!!!!!!!!!!

\subsection{The Main Algorithm}

From the discussion above it is apparent that the additional complexity
due to explicitly handling intercycle dependencies is too difficult to
deal with in a Finite State Machine, and that the ``forecast cycle'' as
a global control system parameter has to be replaced with an independent
``forecast cycle time'' for each task. This devolving of cycle
timing to the individual tasks suggests treating the system as a {\em
simulation} of autonomous proxy objects that represent the external
tasks and interact regardless of cycle time to negotiate
dependencies at run time (i.e.\ by matching completed outputs against
prerequisites). If this can be made to work it provides extraordinary
power and flexibility because it treats all dependencies equally and it
makes any convoluted task scheduling logic entirely disappear: if task
proxy objects can interact indiscriminately then they don't need to know
{\em who} is supposed to satisfy their prerequisites and they can be
defined without reference to the other tasks in the system (except of
course that some other task(s) must exist that will satisfy their
prerequisites).  Existing tasks could be taken out of the system, or new
ones added, without changing the control system in any other way.
Further, by means of object polymorphism\footnote{Polymorphism is the
ability of one type to appear as and be used like another type. In OOP
languages with inheritance, this usually refers to the ability to treat
derived class objects as if they were members of a base class so that,
for instance, a group of mixed-type objects can all be treated as
members of a common base class while retaining their specialized derived
class behaviour.} the control system can be designed to automatically
handle any future task so long as it is derived from (inherits the
properties of) the original task base class.

The following simple description should be sufficient to enable the
reader to understand how the algorithm achieves optimal forecast
cycle-independent metascheduling. Everything else is arguably just
implementation, although some important aspects of that are not trivial
and will be discussed later.

\begin{itemize}
    \item The control system maintains a pool of autonomous {\em task
        proxy objects} that represent each real task. 
       
    \item The internal state of a task proxy object must reflect that
        of the real task it represents. This state information includes:

        \begin{itemize}

            \item task proxy object name.

            \item associated external (real) task.  

            \item owner of the real task, if necessary (who the task
                should run as).

            \item UTC {\em forecast cycle time}, e.g. $2010012418$
        
            \item current execution status: {\em waiting}, {\em running}, 
                {\em finished}, or {\em failed}. 

            \item a list of prerequisites and whether or not they are
                satisfied yet, e.g.\ {\em file FOO is ready}. 

            \item a list of outputs completed so far, e.g.\ {\em file
                FOO is ready}.

        \end{itemize}
       
    \item A task proxy object can launch its associated external task
        when all of its prerequisites are satisfied.

    \item A task proxy object can interact with other task proxy
        objects (regardless of cycle time; all dependencies are now
        equal) to determine if any of their completed outputs can
        satisfy any of its prerequisites.

    \item The control system gets the task pool to interact and
        negotiate dependencies whenever any new output is reported.
 
    \item A task proxy object must exist by the time it is needed to
        interact with other tasks, and must not cease to exist before
        it is no longer needed.

\end{itemize}

\subsubsection{Main Loop}

{\small
\noindent
\rule{5cm}{.2mm}
\begin{lstlisting}
while True:

   if task_base.state_changed:
       # PROCESS ALL TASKS whenever one has changed state
       # as a result of a remote task message coming in. 
       #---
       task_pool.process_tasks()
       task_pool.dump_state()
       if task_pool.all_finished():
           clean_shutdown( "ALL TASKS FINISHED" )

    # REMOTE METHOD HANDLING; handleRequests() returns 
    # after one or more remote method invocations are 
    # processed (these are not just task messages, hence 
    # the use of task_base.state_changed above).
    #---
    task_base.state_changed = False
    pyro_daemon.handleRequests( timeout = None )

# END MAIN LOOP
\end{lstlisting}
}


\label{sec-task-messaging}


\subsection{Task Proxy Object Life Cycle}

Task proxy creation and destruction must be managed so that, in a
continuously running system, they exist when needed, but do not exist
for too long before they are needed, and cease to exist soon after they
are no longer needed.

\subsubsection{Task Creation}

A task proxy object needs to exist, at the latest, by the time that all
of its prerequisites have been satisfied.  The earliest a task can run
is governed chiefly by whether it depends on its previous
instance (in which case it can potentially run as soon as its previous
instance has finished\footnote{Or when it has generated its background
state for the next instance, at least.}) or not (in which case it can
potentially run in parallel with its previous instance). This
information is specific to the task type so the best place to hold it is
in the task proxy class definitions. 

New tasks are therefore created after their previous instance {\em
spawns}; for forecast models this happens when the previous instance
finishes\footnote{But see previous footnote}; and otherwise as soon as
the previous instance starts running. This ensures that a task cannot
run before its previous instance without use of explicit intercycle
prerequisites that would require special treatment at startup (when
there is no previous cycle). Tasks are not deleted immediately on
abdication (see below).

\subsubsection{Removing Spent Tasks} 

A task is spent if it finished {\em and} no longer needed to satisfy the
prequisites of any other task. Most tasks are only needed by other
cotemporal downstream tasks; these can be removed when they are finished
{\em and} older than the oldest non-finished task. For rare cases that
are needed by tasks in later cycle times (e.g.\ nzlam post
processing: multiple hourly topnet tasks need the same most recent
previously finished 06 or 18Z nzlam post processing task), each
non-finished task reports its {\em cutoff cycle time} which is the
oldest cycle time that may contain tasks still needed to satisfy its
own prerequisites (if it is waiting) or those of its immediate
post-abdication successor (if it is running already), then the task
manager can then kill any finished tasks that are also older than the
oldest task cutoff time.

\subsubsection{Removing Lame Tasks} 

Tasks that will never run (because their prerequisites cannot be
satisfied by any other task in the system) are removed from the {\em
oldest batch} of tasks.  If not removed they would prevent the spent
task deletion algorithm from working properly. Lame tasks can only be
detected in the oldest task batch; in younger batches some tasks may yet
appear as their predecessors spawn.

Lame tasks are spawned rather than just deleted, because their
descendents will not necessarily be lame: e.g.\ if the system is started
at 12Z with topnet turned on, all topnet tasks from 12Z through 17Z will
be valid but lame, because they will want to take input from a
non-existent nzlam\_post from 06Z prior to startup. However, the
presence of lame tasks may indicate user error: e.g.\ if you forget
to turn on task type $foo$ that supplies input to task type $bar$,
any instance of $bar$ will be lame.

\subsection{Constraining The System}

No task is allowed to get more than 48 hours (user configurable) ahead
of the slowest task in the system (with respect to cycle time).


\subsection{Coupling Task Proxies to Tasks} 

Our task proxy objects must keep track of progress in their external
counterparts. Most task prerequisites are just files generated by other
tasks, so it is tempting to have the controller use the appearance of
expected new output files as a proxy for task progress. But we have to
be sure that a newly detected file is complete, not just that it exists,
and it is difficult to do this in an OS-independent way (using {\em
inotify} on Linux, for example.). 
%On Linux one could insist that every completed output file is
%immediately renamed by the generating task, and have the controller use
%{\em inotify} to watch for the sudden appearance of the new file
%(because file rename operations are atomic when the source and target
%are on the same file system) [REF: Simon, if he wants]. But this is not
%platform independent, and most forecast systems run on heterogeneous
%distributed hardware. 
More importantly though, prerequisites are not necessarily single files:
a task could conceivably depend on completion of a large set of files, a
database update, or a data transfer by remote procedure call, for
instance. Consequently we chose to use a high level messaging system for
communication between external tasks and the control system. This is
platform independent and allows tasks to be triggered off any
conceivable condition. For example, rather than detecting the existence
of the file {\em FOO}, the controller would receive a message saying
{\em file FOO is ready}, or similar, from the task that has
just generated the file.  There is no need for the control system itself
does to verify that the message is true (i.e. that file {\em FOO}
really does exist) because any downstream task that
depends on file {\em FOO} must necessarily do that itself, and error 
conditions can be reported back to the controller, and possibly to a
separate monitoring system as well, at that point.

The Python Remote Object Protocal (Pyro) allows external programs to
communicate directly, across the network, with specific objects inside
the running controller. This means that tasks can communicate directly
with their own proxy objects, obviating the need for any any internal
message brokering mechanism in the control system.    

Each task must express its prerequisites (i.e.\ its dependence on
upstream tasks) as a text string, for example ``file X is ready'', or
``task X has completed'', or ``task X has completed writing all Y
files'', and must send messages of the same kind back to the controller
to indicate when it has reached an important waypoint or completed
generated any important outputs.  


\subsection{Task Definition}
blah.

\subsection{Pyro}
blah.

\subsection{Pure Simulation Mode}

The dynamic metascheduling algorithm is essentially a simulation of an
interacting task set in which the state of each task proxy object is
coupled to that of the real task it represents. In addition, task proxy
state changes occur in response to {\em messages} rather than, say,
actual detection of newly generated input files.  This suggests
a {\em dummy mode} in which each configured external task is replaced by
an instance of an external dummy program that masquerades as the real
task by reporting completion of each of its outputs in turn (task output
lists can be exposed to other programs, namely to the dummy task
program, through Pyro RPC calls). As far as the control system is
concerned this is indistinguishable from real operation, except that
external dummy tasks are less likely to be delayed by resource
contention, and the dummy mode can be run according to an accelerated
clock, rather than real time, for quick testing.  Dummy tasks therefore
complete in approximately the same dummy clock time as the real tasks do
in real time. An initial dummy clock offset relative to the initial
cycle time can also be specified, which allows simulation of the
transition between catch up and real time operation, and vice versa. Log
messages are stamped with dummy clock time instead of real time.

The same script is used for all external dummy tasks but it has special
behaviour in certain cases: the dummy downloader ``waits for incoming
files'' until 3:15 past its cycle time, and the dummy topnet ``waits
for stream flow data'' until 0:15 past its cycle time.

The dummy clock can be bumped forward a number of hours by remote
control, while the system is running. This affects the postrequisite
timing of running tasks correctly, but if it causes a running task to
finish immediately the next task in line will still start from the
beginning no matter how big the bump.



%some tasks, such as those that wait on external input data, and tide
%models, may have no upstream dependencies at all.

%This could be done by checking for the existence of required inputs
%directly, or by monitoring the state of the other tasks that are known
%to provide the inputs in each case (are they finished yet?).  

%The control program thus remains simple and generic, regardless of the
%number of tasks or the complexity of their interdependencies; it simply
%manages a set of tasks that are all individually configured as if they
%were to run in isolation.\footnote{The system manager does of course
%have to ensure that the configured task pool is self consistent, i.e.\
%that each task's prerequisites will be satisfied by some other task(s)
%in the system.}
%The total absence of explicit scheduling logic makes this method
%extremely flexible and extensible.\footnote{To extend the system, one
%simply derives a new class definition that lists the new task's
%prerequisites and outputs. The new task will automatically run at the
%right time, i.e.\ when its prerequisites have been satisfied by some
%other task(s) in the system.}


\subsection{Applicability}

The object oriented dynamic metascheduling concept is quite general and
could in principle be implemented for any set of interdependent tasks.
cylc, however, is specialized toward cycling forecast systems in that
each task must have an associated {\em forecast cycle time} that is
part of a predetermined series for a given task type and is not
necessarily related to the real time at which the task actually runs.  

\subsection{Environment}

EcoConnect operates in a well defined environment so that each real task
knows what its input files look like for a given cycle time
(through filenaming conventions) and where to get them from (e.g.\ from
their own input directories, or upstream output directories).
Consequently the control system does not need to know the location of
important input/output files, just (via messaging) that they exist. In a
less structured environment additional tasks could easily be added to
to move files around as needed. 

\subsection{Startup and Initialization}

An initial cycle time and list of task object names are read in from
the config file, then each task object is created at the initial
cycle time {\em or} at the first subsequent cycle time that is
valid for the task type. Optionally, we can tell the controller to
reload the current state dump file (which may have been edited); this
will override the configured start time and task list. After startup,
new tasks are created only by {\em abdication} (below).

An initial run through the {\em task processing} code, by virtue of the
fact that the main loop starts with task processing, causes tasks with
no prerequisites (e.g.\ {\em downloader}) to enter the {\em running}
state and launch their external tasks immediately. Otherwise ({\em or}
if there are no tasks that lack prerequisites) nothing will happen.


\subsection{Task Interaction} 

Each task keeps track of which of its postrequisites are completed, and
asks the other tasks if they can satisfy any of its prerequisites. 

{\small
\noindent
\rule{5cm}{.2mm}
\begin{lstlisting}
class task_pool( Pyro.core.ObjBase ):
    # ...
    def interact( self ):
        # get each task to ask all the others if 
        # they can satisfy its prerequisites
        #--
        for task in self.tasks:
            task.get_satisfaction( self.tasks )
    # ...
\end{lstlisting}
}

\subsection{Running Tasks}

Each task object can launch its associated external task, and enter the
{\em running} state if its prerequisites are all satisfied, any existing
older tasks of the same type are already {\em finished}, and fewer than
{\em MAX\_ RUNAHEAD} finished tasks of the same type still exist (this
stops tasks with no prerequisites from running ahead indefinitely).

\subsection{Pyro Remote Method Calls}

The Pyro request handling loop executes remote method calls coming in
from external tasks, and returns after at least one call was handled.
Pyro must be run in non-default single-threaded mode (see Appendix
\ref{pyro-appendix}).

\subsection{Dumping State} 

The current state (waiting, running, or finished) of each task is
written out to the {\em state dump file}.  This provides a running
snapshot of the system as it runs, and just prior to shutdown or
failure. The controller can optionally start up by loading the state
dump (which can be edited first). Any 'running' tasks are reloaded in
the 'waiting' state.


