%\documentclass[11pt,a4paper]{report}
\documentclass[11pt,a4paper]{article}
%\documentclass[11pt,a4paper]{amsart}

\usepackage{listings}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{graphicx}

\lstset{language=Python}

\title{An Optimal Dynamic Metascheduler for Continuously Cycling
Multi-Model Forecasting Systems}

\author{Hilary Oliver, NIWA}

\begin{document}

\maketitle

\begin{abstract}

    A metascheduler is a control system that determines when members of
    a set of tasks with dependencies are ready to be scheduled for
    execution.\footnote{The term can also refer to {\it a single
    aggregate view of multiple Distributed Resource Managers}, which is
    not the topic of this paper.} Environmental forecasting systems
    consist of cycling (indefinitely repeating) task sets that have
    dependencies within and between cycles. Existing forecast control
    systems ignore intercycle dependencies, which greatly increase a
    system's apparent complexity, because they are automatically
    satisfied in normal real time operation. This assumes a linear
    sequence of distinct forecast cycles at all times, however, which is
    very inefficient when catching up from operational delays or running
    historical case studies; then the driving data for the next cycle(s)
    are available in advance and intercycle dependencies may allow tasks
    from many different cycles to run at once.
    %particularly for systems that have little down time between
    %forecast cycles.  
    The metascheduler presented in this paper, {\em cycon}, is able to
    schedule tasks for execution as soon as their prerequisites are
    satisfied, regardless of forecast cycle, to achieve optimal
    throughput in all modes of operation. It is also extremely flexible
    and widely applicable because it uses no system-specific scheduling
    logic: each task simply registers its own prerequisites and reports
    its own outputs, and dependencies are negotiated dynamically so that
    correct execution order emerges naturally at run time. {\em cycon}
    is easily interfaced to existing tasks (models etc.), new tasks will
    automatically run at the right time, and control of any configured
    task set can be fully tested without running the real tasks. It is
    written in Python and uses the Python Remote Object Protocol ({\em
    Pyro}) to control tasks on multiple platforms at once. 

\end{abstract}

\pagebreak
\tableofcontents
\pagebreak

\section{Cycling Multi-Model Forecasting Systems}

Continuously cycling multi-model environmental forecasting operations
generate forecast products at regular intervals using a set of linked
scientific models and associated data processing tasks\footnote{A {\em
task} is any group of processes treated as a single entity for
scheduling purposes.} driven by real time external data (observations
and/or forecast fields from another operation). The EcoConnect
Forecasting System at the National Institute of Water and Atmospheric
Research (NIWA) in New Zealand takes real time atmospheric and stream
flow observations, and operational global weather forecasts from the UK
Met Office, and uses these to drive global sea state and regional data
assimilating weather models, which in turn drive regional sea state,
storm surge, and catchment river models, plus tide prediction, and a
large number of associated data collection, quality control,
preprocessing, postprocessing, product generation, and archiving
tasks.\footnote{Plans for EcoConnect include additional deterministic
regional weather models and a statistical ensemble of weather models.}
In real time operation, the arrival of new external driving data
initiates a new {\em forecast cycle} consisting of a set of tasks with a
common {\em forecast reference time}, the nominal start time of the new
model forecasts. Not all tasks necessarily run in every cycle: the
regional weather model in EcoConnect runs four times daily in the 00,
06, 12, and 18 UTC cycles, but it supplies surface pressures to a
regional storm surge model that runs only twice daily, in the 00 and 12
UTC cycles, and precipitation accumulations to catchment river models
that run on an hourly cycle as they assimilate real time stream flow
observations.  EcoConnect runs on heterogenous distributed hardware,
including a massively parallel supercomputer and several Linux servers. 

\section{Control Systems and Dependencies}

A control system should ideally schedule tasks for execution as soon as
they are ready, but not before.  This is the best that can be done
whether or not the scheduled tasks always begin executing immediately,
which is determined by the batch queue scheduler(s). A task is {\em
ready} to execute when its prerequisites have been satisfied by other
tasks in the system,\footnote{Prerequisites that are external to the
system can be reformulated as outputs of tasks that wait on an external
events.} usually by generating, or otherwise making available, required
input data. The dependencies between tasks do not respect forecast cycle
boundaries; they can exist between different tasks in the same cycle
(e.g.\ a sea state forecast might depend on wind fields generated by a
cotemporal weather forecast), between instances of the same task in
different cycles (e.g.\ a forecast model generally writes out a
``background model state'' that is used in initializing its next
forecast), and between different tasks in different cycles (e.g.\ the
EcoConnect catchment models run on an hourly cycle as they assimilate
real time streamflow observations, but they depend on precipitation data
from the most recent available 6- or 12-hourly regional weather
forecast).


\subsection{Existing Systems: Sequential Cycling}

In real time operation a new forecast cycle is initiated by arrival of
new external driving data, at which point the previous cycle must be
well finished or the system will suffer increasing delays with each
cycle. Intercycle dependencies are therefore automatically satisfied in
normal real time operation and can be ignored. In fact, as far as the
author is aware, all existing forecast control systems do this and
consequently require strict sequential cycling at all times\footnote{It
is difficult to be sure of this as they are invariably highly
system-specific in-house designs that have not been published.
DAGMan, which is part of the Condor distributed workload management
system [REF:...], is a metascheduler for jobs with dependencies, but it
does not appear to be suited to continuously cycling dependent systems.} 

\begin{figure} 
    \begin{center}
    \includegraphics[width=4cm]{dependencies-one} 
    \end{center}
    \caption{\small Dependency diagram for a single forecast cycle in a
    simple example system. Tasks {\em a, b,} and {\em c} represent
    forecast models, and {\em d, e} and {\em f} are post processing or
    product generation tasks.} 
    \label{fig-dep-one} 
\end{figure} 

Figure \ref{fig-dep-one} shows a dependency diagram, which forms a {\em
Directed Acyclic Graph}, for a single cycle of a simple example system
consisting of three forecast models ({\em a, b,} and {\em c}) and three
post processing or product generation tasks ({\em d, e} and {\em f}).
Within a forecast cycle each task may depend on one or more other
``upstream'' tasks, and may itself be depended on by one or more other
``downstream'' tasks.  A control system should therefore be capable of
managing, within a single forecast cycle, multiple parallel streams of
execution that branch when one task generates output for several
downstream tasks, and merge when one task takes input from several
upstream tasks. 

\begin{figure}
    \begin{center}
        \includegraphics[width=8cm]{timeline-one}
    \end{center}
    \caption{\small Optimal job schedule for two consecutive
cycles of the example system during real time operation. The horizontal
extent of a task bar represents execution time. Vertical sections
through the graph intersect all tasks executing at that time, but the
vertical ordering of tasks is not meaningful.}
    \label{fig-time-one}
\end{figure}

Figure \ref{fig-time-one} shows two consecutive cycles of the optimal
job schedule for the example system, in real time operation, given
execution times represented by the horizontal extent of the task bars.
There is a time gap between cycles as the system waits on new external
driving data, and the schedule is optimal in the sense that every task
begins executing as soon as its prerequisites are satisfied. All tasks
here predicate on upstream tasks {\em finishing} rather than on
intermediate outputs, but this is just to make clearer
diagrams.\footnote{Model background states are written early in a
forecast, assuming the forecast length is much longer than the cycle
period, so in principle forecast models don't actually have to wait on
their previous instance {\em finishing} - the new system could easily
handle this using ``background state ready'' messages instead ``task
finished'' messages to trigger task abdication; more on this later.}   

The simplest metascheduling algorithm is probably one in which the user
must explicitly specify the task execution order and guarantee that it
will not violate any dependencies, perhaps with some crude means of
specifying tasks that can run in parallel as well. This is clearly
sub-optimal and puts a heavy burden on the user. A more sophisticated
design capable of achieving the optimal single cycle job schedule of
Figure \ref{fig-time-one} is a Finite State Machine that, within an
event loop that waits on tasks finishing, enforces a predetermined
non-linear order of events using hard coded scheduling logic: {\em if
task A has just finished, then begin executing tasks B and C}, and so
on. Unfortunately this is highly system-specific and rapidly becomes
convoluted as system complexity increases, inevitably leading to
inflexibility and fragility.  As far as the author is aware all existing
forecast control systems are of one type or the other.
\subsection{Optimal Metascheduling}

When catching up from operational delays, and in historical case
studies, the external driving data for the next cycle(s) are available
in advance and it should be possible to get greatly increased efficiency
by running tasks from multiple cycles at once.  But this can only be
allowed to happen if intercycle dependencies are taken into account, to
ensure that tasks never execute before their inputs have been generated.
Otherwise, if we are restricted to sequential cycling, the best that can
be done in general is to reduce the gap between cycles to zero (a
limited crude overlap of the single cycle job schedule may be possible
for a given set of tasks, but we are interested in automatic optimal
metascheduling for any system).


\begin{figure} \begin{center}
    \includegraphics[width=6cm]{dependencies-two} \end{center}
    \caption{\small Complete dependency graph for the example
    system, assuming the least possible intercycle dependence: the
    forecast models ($a$, $b$, and $c$) depend on their own previous
    instances. The dashed arrows show connections to previous and
    subsequent forecast cycles.} 
    \label{fig-dep-two}
\end{figure}

Figure \ref{fig-dep-two} shows the complete dependency graph for the
example system, assuming the least possible intercycle dependence: each
forecast model ($a$, $b$, and $c$) depends on its own previous instance.

\begin{figure} 
    \begin{center} 
        \includegraphics[width=12cm]{timeline-three}
    \end{center} 
    \caption{\small Job schedules for the example system after a delay
    of almost one whole forecast cycle, when intercycle dependencies are
    taken into account (above the time axis), and when they are not
    (below the time time axis). The colored lines indicate the time that
    each cycle is delayed, and normal ``caught up'' cycles
    are shaded gray.} 
    \label{fig-time-three}
\end{figure} 

Figure \ref{fig-time-three}, below the time axis, shows the effect of an
operational delay of almost one whole forecast cycle on a sequential
cycling system: it takes many cycles to catch up again. Above the time
axis is the optimal schedule that is possible, in principle, when
intercycle dependencies are taken into account: the second cycle after
the delay is hardly affected, and subsequent cycles all run on time.
Note that simply overlapping the single cycle schedules of Figure
\ref{fig-time-one} from the same start point would have resulted in a
dependency violation in task {\em c}.  Lack of ability to do this is a
serious problem for complex operations like EcoConnect that have little
downtime between cycles. 

\begin{figure} 
    \begin{center} 
        \includegraphics[width=8cm]{timeline-two}
    \end{center} 
    \caption{\small Job schedules for the example system in case study
    mode, or after a long delay, when the external driving data are
    available many cycles in advance. Above the time axis is the optimal
    schedule obtained when the system is constrained only by its true
    dependencies, as in Figure \ref{fig-dep-two}, and underneath it is
    the best that can be achieved when intercycle dependencies are
    ignored.} 
    \label{fig-time-two}
\end{figure} 

Similarly, Figure \ref{fig-time-two} shows job schedules for the example
system in case study mode (or when catching up after a very long delay)
when the external driving data are available many cycles in advance.
Task {\em a}, which as the most upstream forecast model is likely to be
a resource intensive atmosphere or ocean model, has no dependence on
cotemporal tasks and can therefore run continuously, regardless of how
much downstream processing is yet to be completed in its own cycle or in
any previous forecast cycle. In practice task {\em a} would depend on
cotemporal upstream tasks that wait on the external driving data, but
they would return immediately when the external data is available in
advance, so the result stands. Other tasks can cycle at regular short
intervals, the interval depending on [CHECK THIS] the task run length
relative to that of its longest cotemporal upstream dependency path. In
this case {\em c} can also run continuously, and consecutive instances
of {\em e}, which has no previous-instance dependence, can overlap.
Thus, even for this very simple example system, tasks from three or four
different cycles can run simultaneously, in principle, at any given
time. 

A new approach is required to achieve this; the hard coded scheduling
logic of the sequential cycling Finite State design above already
suffers from complexity-related problems, and extending it to this more
general system hardly seems feasible. 


\section{Optimal Metascheduling Implementation}

In this section we describe a novel algorithm that achieves continuous
optimal metascheduling as in Figures \ref{fig-time-two} and
\ref{fig-time-three}, and smoothly transitions to a linear sequence of
distinct forecast cycles as in Figure \ref{fig-time-one}, as the system
catches up to real time.  

\subsection{The Main Algorithm}

From the discussion above it is apparent that the additional complexity
due to explicitly handling intercycle dependencies is too difficult to
deal with in a Finite State Machine, and that the ``forecast cycle'' as
a global control system parameter has to be replaced with an independent
``forecast reference time'' for each task. This devolving of cycle
timing to the individual tasks suggests treating the system as a {\em
simulation} of autonomous proxy objects that represent the external
tasks and interact regardless of reference time to negotiate
dependencies at run time (i.e.\ by matching completed outputs against
prerequisites). If this can be made to work it provides extraordinary
power and flexibility because it treats all dependencies equally and it
makes any convoluted task scheduling logic entirely disappear: if task
proxy objects can interact indiscriminately then they don't need to know
{\em who} is supposed to satisfy their prerequisites and they can be
defined without reference to the other tasks in the system (except of
course that some other task(s) must exist that will satisfy their
prerequisites).  Existing tasks could be taken out of the system, or new
ones added, without changing the control system in any other way.
Further, by means of object polymorphism\footnote{Polymorphism is the
ability of one type to appear as and be used like another type. In OOP
languages with inheritance, this usually refers to the ability to treat
derived class objects as if they were members of a base class so that,
for instance, a group of mixed-type objects can all be treated as
members of a common base class while retaining their specialized derived
class behaviour.} the control system can be designed to automatically
handle any future task so long as it is derived from (inherits the
properties of) the original task base class.

The following simple description should be sufficient to enable the
reader to understand how the algorithm achieves optimal forecast
cycle-independent metascheduling. Everything else is arguably just
implementation, although some important aspects of that are not trivial
and will be discussed later.

\begin{itemize}
    \item The control system maintains a pool of autonomous {\em task
        proxy objects} that represent each real task. 
       
    \item The internal state of a task proxy object must reflect that
        of the real task it represents. This state information includes:

        \begin{itemize}

            \item task proxy object name.

            \item associated external (real) task.  

            \item owner of the real task, if necessary (who the task
                should run as).

            \item UTC {\em forecast reference time}, e.g. $2010012418$
        
            \item current execution status: {\em waiting}, {\em running}, 
                {\em finished}, or {\em failed}. 

            \item a list of prerequisites and whether or not they are
                satisfied yet, e.g.\ {\em file FOO is ready}. 

            \item a list of outputs completed so far, e.g.\ {\em file
                FOO is ready}.

        \end{itemize}
       
    \item A task proxy object can launch its associated external task
        when all of its prerequisites are satisfied.

    \item A task proxy object can interact with other task proxy
        objects (regardless of reference time; all dependencies are now
        equal) to determine if any of their completed outputs can
        satisfy any of its prerequisites.

    \item The control system gets the task pool to interact and
        negotiate dependencies whenever any new output is reported.
 
    \item A task proxy object must exist by the time it is needed to
        interact with other tasks, and must not cease to exist before
        it is no longer needed.

\end{itemize}

\subsubsection{Main Loop}

{\small
\noindent
\rule{5cm}{.2mm}
\begin{lstlisting}
while True:

   if task_base.state_changed:
       # PROCESS ALL TASKS whenever one has changed state
       # as a result of a remote task message coming in. 
       #---
       task_pool.process_tasks()
       task_pool.dump_state()
       if task_pool.all_finished():
           clean_shutdown( "ALL TASKS FINISHED" )

    # REMOTE METHOD HANDLING; handleRequests() returns 
    # after one or more remote method invocations are 
    # processed (these are not just task messages, hence 
    # the use of task_base.state_changed above).
    #---
    task_base.state_changed = False
    pyro_daemon.handleRequests( timeout = None )

# END MAIN LOOP
\end{lstlisting}
}


\label{sec-task-messaging}


\subsection{Task Proxy Object Life Cycle}

Task proxy creation and destruction must be managed so that, in a
continuously running system, they exist when needed, but do not exist
for too long before they are needed, and cease to exist soon after they
are no longer needed.

\subsubsection{Task Creation}

A task proxy object needs to exist, at the latest, by the time that all
of its prerequisites have been satisfied.  The earliest a task can run
is governed chiefly by whether it depends on its previous
instance (in which case it can potentially run as soon as its previous
instance has finished\footnote{Or when it has generated its background
state for the next instance, at least.}) or not (in which case it can
potentially run in parallel with its previous instance). This
information is specific to the task type so the best place to hold it is
in the task proxy class definitions. 

New tasks are therefore created after their previous instance {\em
abdicates}; for forecast models this happens when the previous instance
finishes\footnote{But see previous footnote}; and otherwise as soon as
the previous instance starts running. This ensures that a task cannot
run before its previous instance without use of explicit intercycle
prerequisites that would require special treatment at startup (when
there is no previous cycle). Tasks are not deleted immediately on
abdication (see below).

\subsubsection{Removing Spent Tasks} 

A task is spent if it finished {\em and} no longer needed to satisfy the
prequisites of any other task. Most tasks are only needed by other
cotemporal downstream tasks; these can be removed when they are finished
{\em and} older than the oldest non-finished task. For rare cases that
are needed by tasks in later reference times (e.g.\ nzlam post
processing: multiple hourly topnet tasks need the same most recent
previously finished 06 or 18Z nzlam post processing task), each
non-finished task reports its {\em cutoff reference time} which is the
oldest reference time that may contain tasks still needed to satisfy its
own prerequisites (if it is waiting) or those of its immediate
post-abdication successor (if it is running already), then the task
manager can then kill any finished tasks that are also older than the
oldest task cutoff time.

\subsubsection{Removing Lame Tasks} 

Tasks that will never run (because their prerequisites cannot be
satisfied by any other task in the system) are removed from the {\em
oldest batch} of tasks.  If not removed they would prevent the spent
task deletion algorithm from working properly. Lame tasks can only be
detected in the oldest task batch; in younger batches some tasks may yet
appear as their predecessors abdicate.

Lame tasks are abdicated rather than just deleted, because their
descendents will not necessarily be lame: e.g.\ if the system is started
at 12Z with topnet turned on, all topnet tasks from 12Z through 17Z will
be valid but lame, because they will want to take input from a
non-existent nzlam\_post from 06Z prior to startup. However, the
presence of lame tasks may indicate user error: e.g.\ if you forget
to turn on task type $foo$ that supplies input to task type $bar$,
any instance of $bar$ will be lame.

\subsection{Constraining The System}

No task is allowed to get more than 48 hours (user configurable) ahead
of the slowest task in the system (with respect to reference time).


\subsection{Coupling Task Proxies to Tasks} 

Our task proxy objects must keep track of progress in their external
counterparts. Most task prerequisites are just files generated by other
tasks, so it is tempting to have the controller use the appearance of
expected new output files as a proxy for task progress. But we have to
be sure that a newly detected file is complete, not just that it exists,
and it is difficult to do this in an OS-independent way (using {\em
inotify} on Linux, for example.). 
%On Linux one could insist that every completed output file is
%immediately renamed by the generating task, and have the controller use
%{\em inotify} to watch for the sudden appearance of the new file
%(because file rename operations are atomic when the source and target
%are on the same file system) [REF: Simon, if he wants]. But this is not
%platform independent, and most forecast systems run on heterogeneous
%distributed hardware. 
More importantly though, prerequisites are not necessarily single files:
a task could conceivably depend on completion of a large set of files, a
database update, or a data transfer by remote procedure call, for
instance. Consequently we chose to use a high level messaging system for
communication between external tasks and the control system. This is
platform independent and allows tasks to be triggered off any
conceivable condition. For example, rather than detecting the existence
of the file {\em FOO}, the controller would receive a message saying
{\em file FOO is ready}, or similar, from the task that has
just generated the file.  There is no need for the control system itself
does to verify that the message is true (i.e. that file {\em FOO}
really does exist) because any downstream task that
depends on file {\em FOO} must necessarily do that itself, and error 
conditions can be reported back to the controller, and possibly to a
separate monitoring system as well, at that point.

The Python Remote Object Protocal (Pyro) allows external programs to
communicate directly, across the network, with specific objects inside
the running controller. This means that tasks can communicate directly
with their own proxy objects, obviating the need for any any internal
message brokering mechanism in the control system.    

Each task must express its prerequisites (i.e.\ its dependence on
upstream tasks) as a text string, for example ``file X is ready'', or
``task X has completed'', or ``task X has completed writing all Y
files'', and must send messages of the same kind back to the controller
to indicate when it has reached an important waypoint or completed
generated any important outputs.  


\subsection{Task Definition}
blah.

\subsection{Pyro}
blah.

\subsection{Pure Simulation Mode}

The dynamic metascheduling algorithm is essentially a simulation of an
interacting task set in which the state of each task proxy object is
coupled to that of the real task it represents. In addition, task proxy
state changes occur in response to {\em messages} rather than, say,
actual detection of newly generated input files.  This suggests
a {\em dummy mode} in which each configured external task is replaced by
an instance of an external dummy program that masquerades as the real
task by reporting completion of each of its outputs in turn (task output
lists can be exposed to other programs, namely to the dummy task
program, through Pyro RPC calls). As far as the control system is
concerned this is indistinguishable from real operation, except that
external dummy tasks are less likely to be delayed by resource
contention, and the dummy mode can be run according to an accelerated
clock, rather than real time, for quick testing.  Dummy tasks therefore
complete in approximately the same dummy clock time as the real tasks do
in real time. An initial dummy clock offset relative to the initial
reference time can also be specified, which allows simulation of the
transition between catch up and real time operation, and vice versa. Log
messages are stamped with dummy clock time instead of real time.

The same script is used for all external dummy tasks but it has special
behaviour in certain cases: the dummy downloader ``waits for incoming
files'' until 3:15 past its reference time, and the dummy topnet ``waits
for stream flow data'' until 0:15 past its reference time.

The dummy clock can be bumped forward a number of hours by remote
control, while the system is running. This affects the postrequisite
timing of running tasks correctly, but if it causes a running task to
finish immediately the next task in line will still start from the
beginning no matter how big the bump.



%some tasks, such as those that wait on external input data, and tide
%models, may have no upstream dependencies at all.

%This could be done by checking for the existence of required inputs
%directly, or by monitoring the state of the other tasks that are known
%to provide the inputs in each case (are they finished yet?).  

%The control program thus remains simple and generic, regardless of the
%number of tasks or the complexity of their interdependencies; it simply
%manages a set of tasks that are all individually configured as if they
%were to run in isolation.\footnote{The system manager does of course
%have to ensure that the configured task pool is self consistent, i.e.\
%that each task's prerequisites will be satisfied by some other task(s)
%in the system.}
%The total absence of explicit scheduling logic makes this method
%extremely flexible and extensible.\footnote{To extend the system, one
%simply derives a new class definition that lists the new task's
%prerequisites and outputs. The new task will automatically run at the
%right time, i.e.\ when its prerequisites have been satisfied by some
%other task(s) in the system.}


\subsection{Applicability}

The object oriented dynamic metascheduling concept is quite general and
could in principle be implemented for any set of interdependent tasks.
cycon, however, is specialized toward cycling forecast systems in that
each task must have an associated {\em forecast reference time} that is
part of a predetermined series for a given task type and is not
necessarily related to the real time at which the task actually runs.  

\subsection{Environment}

EcoConnect operates in a well defined environment so that each real task
knows what its input files look like for a given reference time
(through filenaming conventions) and where to get them from (e.g.\ from
their own input directories, or upstream output directories).
Consequently the control system does not need to know the location of
important input/output files, just (via messaging) that they exist. In a
less structured environment additional tasks could easily be added to
to move files around as needed. 

\subsection{Startup and Initialization}

An initial reference time and list of task object names are read in from
the config file, then each task object is created at the initial
reference time {\em or} at the first subsequent reference time that is
valid for the task type. Optionally, we can tell the controller to
reload the current state dump file (which may have been edited); this
will override the configured start time and task list. After startup,
new tasks are created only by {\em abdication} (below).

An initial run through the {\em task processing} code, by virtue of the
fact that the main loop starts with task processing, causes tasks with
no prerequisites (e.g.\ {\em downloader}) to enter the {\em running}
state and launch their external tasks immediately. Otherwise ({\em or}
if there are no tasks that lack prerequisites) nothing will happen.


\subsection{Task Interaction} 

Each task keeps track of which of its postrequisites are completed, and
asks the other tasks if they can satisfy any of its prerequisites. 

{\small
\noindent
\rule{5cm}{.2mm}
\begin{lstlisting}
class task_pool( Pyro.core.ObjBase ):
    # ...
    def interact( self ):
        # get each task to ask all the others if 
        # they can satisfy its prerequisites
        #--
        for task in self.tasks:
            task.get_satisfaction( self.tasks )
    # ...
\end{lstlisting}
}

\subsection{Running Tasks}

Each task object can launch its associated external task, and enter the
{\em running} state if its prerequisites are all satisfied, any existing
older tasks of the same type are already {\em finished}, and fewer than
{\em MAX\_ RUNAHEAD} finished tasks of the same type still exist (this
stops tasks with no prerequisites from running ahead indefinitely).

\subsection{Pyro Remote Method Calls}

The Pyro request handling loop executes remote method calls coming in
from external tasks, and returns after at least one call was handled.
Pyro must be run in non-default single-threaded mode (see Appendix
\ref{pyro-appendix}).

\subsection{Dumping State} 

The current state (waiting, running, or finished) of each task is
written out to the {\em state dump file}.  This provides a running
snapshot of the system as it runs, and just prior to shutdown or
failure. The controller can optionally start up by loading the state
dump (which can be edited first). Any 'running' tasks are reloaded in
the 'waiting' state.

\appendix

\section{Essential OOP Concepts}

The simplicity of the dynamic scheduling implementation in {\em cycon}
is critically dependent on the {\em polymorphic} nature of the {\em task
objects} in the program.  This section contains a minimal introduction
to these Object Oriented Programming concepts.  Refer to any OOP
reference for more detail.

\subsection{Classes and Objects}

A {\em class} is essentially a generalisation of {\em data type} to
include {\em behaviour} (i.e.\ functions or {\em methods}) as well as
state.  {\em Objects} are more or less self contained {\em instances} of
a class. For example, a $shape$ class could define a $position$ data
member that describes the location of each shape object, a $move()$
method that causes a shape object to alter its position, and a $draw()$
method that causes it to display itself in the right place on screen.

\subsection{Inheritance}

A {\em derived class} or {\em subclass} inherits the properties (methods
and data members) of a {\em base class}. It can also {\em override}
specific base class properties, or add new properties that aren't
present in the base class. Calling a particular method on an object
invokes the object's own class method if one is defined, otherwise the
immediate base class is searched, and so on down to the root of the
inheritance graph. 

For example, we could derive a $circle$ class from $shape$, adding a
`radius' data member and overriding the $draw()$ to get circle objects
to display themselves as actual circles.  Because we didn't override the
$move()$ method, calling $circle.move()$ would invoke the base class
method, $shape.move()$. 


\subsection{Polymorphism}

Polymorphism is the ability of one type to appear as and be used like
another type.  In OOP languages with inheritance, this usually refers to
the ability to treat derived/sub-class objects as if they were members
of a base class.  In particular, a group of mixed-type objects can all
be treated as members of a common base class. For example, we could
construct a list of $shape$ objects from $circles$, $triangles$, and
$squares$; calling $[list member].draw()$ will invoke the right derived
class $draw()$. This is a very powerful mechanism because {\em it allows
unmodified old code to call new code}: if we later derive an entirely
new kind of shape ($hexagon$, say) with it's own unique behaviour, the
existing program, without modification, will process the new objects in
the proper hexagon-specific way.


\section{Threading in Pyro} \label{pyro-appendix}

With Pyro in {\em single threaded mode}, \verb#handleRequests()# returns
after {\em either} a timeout has occurred {\em or} at least one request
(i.e.\ remote method call) was handled. With \verb#timeout = None# this
allows us to process tasks {\em only} after remote method invocations
come in.  Further, we can detect the remote calls that actually change
task states, and thereby drop into the task processing code only when
necessary, which minimizes non-useful output from the task processing
loop (e.g.\ in dummy mode there are a lot of remote calls on the dummy
clock object, which does not alter tasks at all). 

In {\em multithreaded mode}, \verb#handleRequests()# returns immediately
after creating a new request handling thread for a single remote object
and thereafter remote method calls on that object come in asynchronously
in the dedicated thread. This is not good for the dynamic scheduling
algorithm because tasks are only set running in the task processing
block which can be delayed while \verb#handleRequests()# blocks waiting
for a new connection to be established even as messages that warrent
task processing are coming in on existing connections. The only way
around this is to do task processing on \verb#handleRequests()# timeouts
which results in a lot of unnecessary task processing when nothing
important is happening.


\subsection{Interfacing Tasks To cycon}
blah.

\subsection{Utility Programs}
blah.

\subsubsection{Configure System}
blah.

\subsubsection{Remote Control}
blah.

\subsubsection{System Monitors}
blah.

\subsection{Logging}
blah.

\section{Miscellaneous Notes}

\subsection{Product Generation}

Note that ``correct scheduling'' is not necessarily equivalent to
``orderly generation of products by reference time'' (the upstream 
forecast model can run continuously regardless of any downstream
processing that depends on it).

\subsection{Catching Up}

The state of ``catching up'' is a property of the individual tasks, not
the whole system. Where this matters, it can be detected by the relevant
external task. E.g.\ if the external topnet(T) task starts up at real
time t greater than the stream flow data time for T (i.e.\ $T+15$ min),
i.e.\ the required stream flow data is already available, then we're
still in catchup. If, on the other hand, the topnet task finds that it
has to wait for its stream flow data time to arrive, then we're caught
up to real time.  This matters because topnet is allowed to run ahead by
a different amount of time depending on whether we're in catchup mode or
not.

\subsection{Controlling Task Execution}

\begin{itemize}
 \item  prerequisites
 \item artificial prerequisites (e.g.\ make nztide depend on nzlam)
 \item delayed instantiation (a task can't run if it doesn't exist yet).
 \item other contraints based on, for example, the number of previous
 instances that still exist in the system, or how far ahead of the 
 slowest/oldest task we can get.
\end{itemize}

\subsection{fuzzy prequisites}

{\em Exact prerequisites} (most tasks): times are specified exactly,
relative to the task's own reference time.  E.g.\ {\em file foo\_{T}.nc
ready} where T is the task's reference time.

{\em Fuzzy prerequisites} (topnet): a time boundary is specified
relative to the task's own reference time; any task with a reference
time greater than or equal to the boundary time can satisify the
prerequisite.

\end{document}
