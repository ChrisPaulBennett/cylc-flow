%\documentclass[11pt,a4paper]{report}
\documentclass[11pt,a4paper]{article}
%\documentclass[11pt,a4paper]{amsart}

\usepackage{listings}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{graphicx}

\lstset{language=Python}

\title{An Optimal Dynamic Metascheduler for Continuously Cycling
Multi-Model Forecasting Systems}

\author{Hilary Oliver, NIWA}

\begin{document}

\maketitle

\begin{abstract}

    A metascheduler is a control system that determines when members of
    a set of tasks with dependencies are 
    % ready to be scheduled for execution.
    ready to be sent to the batch queue scheduler.\footnote{The term can
    also refer to {\it a single aggregate view of multiple Distributed
    Resource Managers}, which is not the topic of this paper.}
    Environmental forecasting systems consist of an indefinitely
    repeating set of tasks that have dependencies within and between
    cycles. Intercycle dependencies greatly increase the apparent
    complexity of a system, but existing control systems ignore them
    because they are automatically satisfied in normal real time
    operation (where one cycle necessarily finishes before the next
    begins).  When the driving data for the next cycle(s) are available
    in advance, however, such as during catch up from delays or in
    historical case studies, proper handling of intercycle dependencies
    can lead to dramatic gains in efficiency, particularly for systems
    that have little down time between forecast cycles.  Our new dynamic
    metascheduler, {\em iCycle}, treats all dependencies equally and is
    able to schedule tasks for execution as soon as their prerequisites
    are satisfied, regardless of forecast cycle, to achieve optimal
    throughput in all modes of operation. {\em iCycle} is extremely
    flexible and widely applicable because it uses no system-specific
    scheduling logic: each task simply registers its own prerequisites
    and reports its own outputs, and dependencies are negotiated
    dynamically so that correct execution order emerges naturally at run
    time.  Tasks can be switched in and out on the fly, new tasks can be
    added easily, and parallel test systems, or entirely new systems,
    can be built quickly. It is easily interfaced to existing tasks
    (models etc.) and has a simulation mode that completely tests
    control of a configured task set, in accelerated time, without
    running the real external tasks. {\em iCycle} is written in Python
    and uses the Python Remote Object Protocol ({\em Pyro}) to control
    tasks on multiple platforms at once. 

\end{abstract}

\pagebreak
\tableofcontents
\pagebreak

\section{Cycling Multi-Model Forecasting Systems}

Continuously cycling multi-model environmental forecasting operations
generate forecast products at regular intervals using a set of connected
scientific models and associated data processing tasks\footnote{A {\em
task} is any group of processes treated as a single entity for
scheduling purposes.} driven by real time observational data and/or
forecast fields from another operation. The EcoConnect Forecasting
System at the National Institute of Water and Atmospheric Research
(NIWA) in New Zealand takes real time atmospheric and stream flow
observations, and operational global weather forecasts from the UK Met
Office, and uses these to drive global sea state and regional data
assimilating weather models, which in turn drive regional sea state,
storm surge, and catchment river models, plus tide prediction, and a
large number of associated data collection, quality control,
preprocessing, postprocessing, product generation, and archiving
tasks.\footnote{Plans for EcoConnect include additional deterministic
regional weather models and a statistical ensemble of weather models.}
In real time operation, the arrival of new external driving data
initiates a new {\em forecast cycle} consisting of a set of tasks with a
common {\em forecast reference time}, the nominal start time of the new
model forecasts. Not all tasks necessarily run in every cycle: the
regional weather model in EcoConnect runs four times daily in the 00,
06, 12, and 18 UTC cycles, but it supplies surface pressures to a
regional storm surge model that runs only twice daily, in the 00 and 12
UTC cycles, and precipitation accumulations to catchment river models
that run on an hourly cycle as they assimilate real time stream flow
observations.  EcoConnect runs on heterogenous distributed hardware,
including a massively parallel supercomputer and several Linux servers. 

\section{Control Systems and Dependencies}

An ideal forecasting control system would schedule tasks for execution
as soon as they are ready, but not before.
%(in other words, a task must not execute before its inputs have been
%generated by other tasks in the system). 
This is the best that can be done whether or not scheduled tasks can
actually begin executing immediately, which depends on resource
availability as seen by the batch queue scheduler(s). A task is {\em
ready} to execute when its prerequisites have been satisfied by other
tasks in the system,\footnote{Prerequisites that are external to the
system can be reformulated as outputs of tasks that wait on an external
events.} usually by means of generating, or otherwise making available,
required input data. These dependencies typically connect different
tasks in the same forecast cycle (e.g.\ a sea state forecast that
depends on wind fields from a cotemporal weather forecast) and instances
of the same task in consecutive cycles (a forecast model generally
writes out a ``background model state'' that used in initializing its
next forecast).  But dependencies linking different tasks across
multiple forecast cycles are also possible (at least they are if you use
{\em iCycle}!). For example, the EcoConnect catchment models run on an
hourly cycle as they assimilate real time streamflow observations, but
they depend on precipitation data from the most recent available 6- or
12-hourly regional weather forecast.


\subsection{Existing Systems: Sequential Cycling}

In real time operation a new forecast cycle is initiated by the arrival
of the next batch of external driving data. The previous cycle must be
well finished by then, or the system would be increasingly delayed with
each cycle.  If cycles never overlap there is no danger of running a
task before inputs from a previous cycle have been generated, so in real
time operation intercycle dependencies are automatically satisfied and
can be ignored.  As far as the author is aware all existing control
systems do this and therefore require strict sequential cycling at all
times, although it is difficult to be sure of this because they are
invariably highly system-specific in-house designs that are not
documented in the literature.\footnote{DAGMan, which is part of the
Condor distributed workload management system [REF:...], is a
metascheduler for jobs with dependencies, but it does not appear to be
suited to continuous dependent systems.} 

\begin{figure} 
    \begin{center}
    \includegraphics[width=4cm]{dependencies-one} 
    \end{center}
    \caption{\small Dependency diagram for a single forecast cycle in a
    simple example system. Tasks {\em a, b,} and {\em c} represent
    forecast models, and {\em d, e} and {\em f} are post processing or
    product generation tasks.} 
    \label{fig-dep-one} 
\end{figure} 

Figure \ref{fig-dep-one} shows a dependency diagram, which forms a {\em
Directed Acyclic Graph}, for a single cycle of a simple example system
consisting of three forecast models ({\em a, b,} and {\em c}) and three
post processing or product generation tasks ({\em d, e} and {\em f}).
Within a forecast cycle each task may depend on one or more other
``upstream'' tasks, and may itself be depended on by one or more other
``downstream'' tasks.  A control system should therefore be capable of
managing, within a single forecast cycle, multiple parallel streams of
execution that branch when one task generates output for several
downstream tasks, and merge when one task takes input from several
upstream tasks. 

\begin{figure}
    \begin{center}
        \includegraphics[width=8cm]{timeline-one}
    \end{center}
    \caption{\small Optimal job schedule for two consecutive cycles of
    the example system during real time operation. Execution times are
    represented by the horizontal extent of the task bars, and the gray
    area is downtime while the system waits on new external driving
    data.  A vertical section through the graph will intersect all tasks
    executing at the same time, but the vertical ordering of tasks is
    not meaningful.}
    \label{fig-time-one}
\end{figure}

Figure \ref{fig-time-one} shows two consecutive cycles of the optimal
job schedule for the example system in real time operation given task
execution times represented by the horizontal extent of the bars. 
There is a time gap between cycles as the system waits on new external
driving data, and the schedule is optimal in the sense that every task
begins executing as soon as its prerequisites are satisfied.  We have
assumed here that all tasks predicate on upstream tasks {\em finishing}
rather than on intermediate outputs such as {\em file FOO is ready}, but
this is only to make for clearer diagrams.\footnote{Model background
states are written early in a forecast, assuming the forecast length is
much longer than the cycle period, so in principle forecast models don't
actually have to wait on their previous instance {\em finishing} - the
new system could easily handle this using ``background state ready''
messages instead ``task finished'' messages to trigger task abdication;
more on this later.}   

The obvious control system design for Figure \ref{fig-dep-one} is a
Finite State Machine that, within an event loop responding to tasks
finishing and/or completing important outputs, enforces a predetermined
non-linear order of events using hard coded scheduling logic: {\em if
task A has just, then begin executing tasks B and C} and so on. As far
as the author is aware existing forecast control systems are all of this
type, or an even simpler design in which the user has to explicitly
define the task execution order, perhaps with some crude means of
specifying functional parallelism. Even for a single forecast cycle,
Finite State implementations can be optimal within a single forecast
cycle but suffer from being highly system-specific, and the hard coded
logic becomes increasingly convoluted as system complexity increases,
inevitably leading to inflexibility and fragility (consider adding a new
task with dependencies into an existing system).

\subsection{Optimal Metascheduling}

If the external driving data for the next cycle(s) are available in
advance, as is frequently the case after operational delays or in
historical case studies, it should be possible to run tasks from one or
cycles simultaneously.  But the best that can be done with strict
sequential cycling, {\em in general}, is to reduce the gap between
cycles to zero;\footnote{For a particular task set it may be possible to
overlap the single cycle job schedules, to a limited degree, without
violating any dependencies; but that is necessarily system-dependent and
suboptimal, and we are interested in automatic optimal interleaving of
forecast cycles regardless of the task list.} this is a serious problem
for complex operations like EcoConnect that have little downtime between
cycles and consequently take much longer than necessary to catch up
after delays. 

To realize the potential for functional parallelism, all intercycle
dependencies must be taken into account so that tasks are never executed
before inputs have been generated by tasks in earlier cycles. 
\begin{figure} \begin{center}
    \includegraphics[width=6cm]{dependencies-two} \end{center}
    \caption{\small Complete dependency graph for the example
    system, assuming the least possible intercycle dependence: the
    forecast models ($a$, $b$, and $c$) depend on their own previous
    instances. The dashed arrows show connections to previous and
    subsequent forecast cycles.} 
    \label{fig-dep-two}
\end{figure}
Figure \ref{fig-dep-two} shows the complete dependency graph for the
example system, assuming the least possible intercycle dependence: each
forecast model ($a$, $b$, and $c$) depends on its own previous instance. 
Our nice linear sequence of distinct forecast cycles has been
transformed into a continuous complex network of tasks, and extending
already convoluted hard coded finite state logic to this would hardly be
feasible.

\begin{figure} 
    \begin{center} 
        \includegraphics[width=12cm]{timeline-three}
    \end{center} 
    \caption{\small Job schedules for the example system after a 
    delay of almost one whole forecast cycle, when intercycle
    dependencies are taken into account (above the time axis), and when
    they are not (below the time time axis). The colored lines indicate
    the time that each cycle is delayed, and normal ``caught up'' cycles
    are shaded gray.} 
    \label{fig-time-three}
\end{figure} 

Figure \ref{fig-time-three} shows job schedules for the example system
after a delay of almost one forecast cycle. With strict sequential
cycling it takes many cycles for the system to catch back up to real
time operation. When intercycle dependencies are taken into account, on
the other hand, the second cycle after the delay is hardly affected, and
subsequent cycles all run on time. Note that simply overlapping the
single cycle schedules of Figure \ref{fig-time-one} from the same start
point would have resulted in a dependency violation in task {\em c}.

\begin{figure} 
    \begin{center} 
        \includegraphics[width=8cm]{timeline-two}
    \end{center} 
    \caption{\small Job schedules for the example system in case study
    mode, or after a long delay, when the external driving data are
    available many cycles in advance. Above the time axis is the optimal
    schedule obtained when the system is constrained only by its true
    dependencies, as in Figure \ref{fig-dep-two}, and underneath it is
    the best that can be achieved when intercycle dependencies are
    ignored.} 
    \label{fig-time-two}
\end{figure} 

Similarly, Figure \ref{fig-time-two} shows job schedules for the example
system in case study mode (or when catching up after a very long delay)
when the external driving data are available many cycles in advance.
Task {\em a}, which as the most upstream forecast model is likely to be
a resource intensive atmosphere or ocean model, has no dependence on
cotemporal tasks and can therefore run continuously, regardless of how
much downstream processing is yet to be completed in its own cycle or in
any previous forecast cycle. In practice task {\em a} would depend on
cotemporal upstream tasks that wait on the external driving data, but
they would return immediately when the external data is available in
advance, so the result stands. Other tasks can cycle at regular short
intervals, the interval depending on [CHECK THIS] the task run length
relative to that of its longest cotemporal upstream dependency path. In
this case {\em c} can also run continuously, and consecutive instances
of {\em e}, which has no previous-instance dependence, can overlap.
Thus, even for this very simple example system, tasks from three or four
different cycles can run simultaneously, in principle, at any given
time. 

\section{Optimal Metascheduling Implementation}

In this section we describe a novel algorithm that achieves continuous
optimal metascheduling as in Figures \ref{fig-time-two} and
\ref{fig-time-three}, and smoothly transitions to a linear sequence of
distinct forecast cycles as in Figure \ref{fig-time-one}, as the system
catches up to real time.  

\subsection{The Main Algorithm}

From the discussion above it is apparent that the additional complexity
due to explicitly handling intercycle dependencies is too difficult to
deal with in a Finite State Machine, and that the ``forecast cycle'' as
a global control system parameter has to be replaced with an independent
``forecast reference time'' for each task. This devolving of cycle
timing to the individual tasks suggests treating the system as a {\em
simulation} of autonomous proxy objects that represent the external
tasks and interact regardless of reference time to negotiate
dependencies at run time (i.e.\ by matching completed outputs against
prerequisites). If this can be made to work it provides extraordinary
power and flexibility because it treats all dependencies equally and it
makes any convoluted task scheduling logic entirely disappear: if task
proxy objects can interact indiscriminately then they don't need to know
{\em who} is supposed to satisfy their prerequisites and they can be
defined without reference to the other tasks in the system (except of
course that some other task(s) must exist that will satisfy their
prerequisites).  Existing tasks could be taken out of the system, or new
ones added, without changing the control system in any other way.
Further, by means of object polymorphism\footnote{Polymorphism is the
ability of one type to appear as and be used like another type. In OOP
languages with inheritance, this usually refers to the ability to treat
derived class objects as if they were members of a base class so that,
for instance, a group of mixed-type objects can all be treated as
members of a common base class while retaining their specialized derived
class behaviour.} the control system can be designed to automatically
handle any future task so long as it is derived from (inherits the
properties of) the original task base class.

The following simple description should be sufficient to enable the
reader to understand how the algorithm achieves optimal forecast
cycle-independent metascheduling. Everything else is arguably just
implementation, although some important aspects of that are not trivial
and will be discussed later.

\begin{itemize}
    \item The control system maintains a pool of autonomous {\em task
        proxy objects} that represent each real task. 
       
    \item The internal state of a task proxy object must reflect that
        of the real task it represents. This state information includes:

        \begin{itemize}

            \item task proxy object name.

            \item associated external (real) task.  

            \item owner of the real task, if necessary (who the task
                should run as).

            \item UTC {\em forecast reference time}, e.g. $2010012418$
        
            \item current execution status: {\em waiting}, {\em running}, 
                {\em finished}, or {\em failed}. 

            \item a list of prerequisites and whether or not they are
                satisfied yet, e.g.\ {\em file FOO is ready}. 

            \item a list of outputs completed so far, e.g.\ {\em file
                FOO is ready}.

        \end{itemize}
       
    \item A task proxy object can launch its associated external task
        when all of its prerequisites are satisfied.

    \item A task proxy object can interact with other task proxy
        objects (regardless of reference time; all dependencies are now
        equal) to determine if any of their completed outputs can
        satisfy any of its prerequisites.

    \item The control system gets the task pool to interact and
        negotiate dependencies whenever any new output is reported.
 
    \item A task proxy object must exist by the time it is needed to
        interact with other tasks, and must not cease to exist before
        it is no longer needed.

\end{itemize}

\subsubsection{Main Loop}

{\small
\noindent
\rule{5cm}{.2mm}
\begin{lstlisting}
while True:

   if task_base.state_changed:
       # PROCESS ALL TASKS whenever one has changed state
       # as a result of a remote task message coming in. 
       #---
       task_pool.process_tasks()
       task_pool.dump_state()
       if task_pool.all_finished():
           clean_shutdown( "ALL TASKS FINISHED" )

    # REMOTE METHOD HANDLING; handleRequests() returns 
    # after one or more remote method invocations are 
    # processed (these are not just task messages, hence 
    # the use of task_base.state_changed above).
    #---
    task_base.state_changed = False
    pyro_daemon.handleRequests( timeout = None )

# END MAIN LOOP
\end{lstlisting}
}


\label{sec-task-messaging}


\subsection{Task Proxy Object Life Cycle}

Task proxy creation and destruction must be managed so that, in a
continuously running system, they exist when needed, but do not exist
for too long before they are needed, and cease to exist soon after they
are no longer needed.

\subsubsection{Task Creation}

A task proxy object needs to exist, at the latest, by the time that all
of its prerequisites have been satisfied.  The earliest a task can run
is governed chiefly by whether it depends on its previous
instance (in which case it can potentially run as soon as its previous
instance has finished\footnote{Or when it has generated its background
state for the next instance, at least.}) or not (in which case it can
potentially run in parallel with its previous instance). This
information is specific to the task type so the best place to hold it is
in the task proxy class definitions. 

New tasks are therefore created after their previous instance {\em
abdicates}; for forecast models this happens when the previous instance
finishes\footnote{But see previous footnote}; and otherwise as soon as
the previous instance starts running. This ensures that a task cannot
run before its previous instance without use of explicit intercycle
prerequisites that would require special treatment at startup (when
there is no previous cycle). Tasks are not deleted immediately on
abdication (see below).

\subsubsection{Removing Spent Tasks} 

A task is spent if it finished {\em and} no longer needed to satisfy the
prequisites of any other task. Most tasks are only needed by other
cotemporal downstream tasks; these can be removed when they are finished
{\em and} older than the oldest non-finished task. For rare cases that
are needed by tasks in later reference times (e.g.\ nzlam post
processing: multiple hourly topnet tasks need the same most recent
previously finished 06 or 18Z nzlam post processing task), each
non-finished task reports its {\em cutoff reference time} which is the
oldest reference time that may contain tasks still needed to satisfy its
own prerequisites (if it is waiting) or those of its immediate
post-abdication successor (if it is running already), then the task
manager can then kill any finished tasks that are also older than the
oldest task cutoff time.

\subsubsection{Removing Lame Tasks} 

Tasks that will never run (because their prerequisites cannot be
satisfied by any other task in the system) are removed from the {\em
oldest batch} of tasks.  If not removed they would prevent the spent
task deletion algorithm from working properly. Lame tasks can only be
detected in the oldest task batch; in younger batches some tasks may yet
appear as their predecessors abdicate.

Lame tasks are abdicated rather than just deleted, because their
descendents will not necessarily be lame: e.g.\ if the system is started
at 12Z with topnet turned on, all topnet tasks from 12Z through 17Z will
be valid but lame, because they will want to take input from a
non-existent nzlam\_post from 06Z prior to startup. However, the
presence of lame tasks may indicate user error: e.g.\ if you forget
to turn on task type $foo$ that supplies input to task type $bar$,
any instance of $bar$ will be lame.

\subsection{Constraining The System}

No task is allowed to get more than 48 hours (user configurable) ahead
of the slowest task in the system (with respect to reference time).


\subsection{Coupling Task Proxies to Tasks} 

Our task proxy objects must keep track of progress in their external
counterparts. Most task prerequisites are just files generated by other
tasks, so it is tempting to have the controller use the appearance of
expected new output files as a proxy for task progress. But we have to
be sure that a newly detected file is complete, not just that it exists,
and it is difficult to do this in an OS-independent way (using {\em
inotify} on Linux, for example.). 
%On Linux one could insist that every completed output file is
%immediately renamed by the generating task, and have the controller use
%{\em inotify} to watch for the sudden appearance of the new file
%(because file rename operations are atomic when the source and target
%are on the same file system) [REF: Simon, if he wants]. But this is not
%platform independent, and most forecast systems run on heterogeneous
%distributed hardware. 
More importantly though, prerequisites are not necessarily single files:
a task could conceivably depend on completion of a large set of files, a
database update, or a data transfer by remote procedure call, for
instance. Consequently we chose to use a high level messaging system for
communication between external tasks and the control system. This is
platform independent and allows tasks to be triggered off any
conceivable condition. For example, rather than detecting the existence
of the file {\em FOO}, the controller would receive a message saying
{\em file FOO is ready}, or similar, from the task that has
just generated the file.  There is no need for the control system itself
does to verify that the message is true (i.e. that file {\em FOO}
really does exist) because any downstream task that
depends on file {\em FOO} must necessarily do that itself, and error 
conditions can be reported back to the controller, and possibly to a
separate monitoring system as well, at that point.

The Python Remote Object Protocal (Pyro) allows external programs to
communicate directly, across the network, with specific objects inside
the running controller. This means that tasks can communicate directly
with their own proxy objects, obviating the need for any any internal
message brokering mechanism in the control system.    

Each task must express its prerequisites (i.e.\ its dependence on
upstream tasks) as a text string, for example ``file X is ready'', or
``task X has completed'', or ``task X has completed writing all Y
files'', and must send messages of the same kind back to the controller
to indicate when it has reached an important waypoint or completed
generated any important outputs.  


\subsection{Task Definition}
blah.

\subsection{Pyro}
blah.

\subsection{Pure Simulation Mode}

The dynamic metascheduling algorithm is essentially a simulation of an
interacting task set in which the state of each task proxy object is
coupled to that of the real task it represents. In addition, task proxy
state changes occur in response to {\em messages} rather than, say,
actual detection of newly generated input files.  This suggests
a {\em dummy mode} in which each configured external task is replaced by
an instance of an external dummy program that masquerades as the real
task by reporting completion of each of its outputs in turn (task output
lists can be exposed to other programs, namely to the dummy task
program, through Pyro RPC calls). As far as the control system is
concerned this is indistinguishable from real operation, except that
external dummy tasks are less likely to be delayed by resource
contention, and the dummy mode can be run according to an accelerated
clock, rather than real time, for quick testing.  Dummy tasks therefore
complete in approximately the same dummy clock time as the real tasks do
in real time. An initial dummy clock offset relative to the initial
reference time can also be specified, which allows simulation of the
transition between catch up and real time operation, and vice versa. Log
messages are stamped with dummy clock time instead of real time.

The same script is used for all external dummy tasks but it has special
behaviour in certain cases: the dummy downloader ``waits for incoming
files'' until 3:15 past its reference time, and the dummy topnet ``waits
for stream flow data'' until 0:15 past its reference time.

The dummy clock can be bumped forward a number of hours by remote
control, while the system is running. This affects the postrequisite
timing of running tasks correctly, but if it causes a running task to
finish immediately the next task in line will still start from the
beginning no matter how big the bump.



%some tasks, such as those that wait on external input data, and tide
%models, may have no upstream dependencies at all.

%This could be done by checking for the existence of required inputs
%directly, or by monitoring the state of the other tasks that are known
%to provide the inputs in each case (are they finished yet?).  

%The control program thus remains simple and generic, regardless of the
%number of tasks or the complexity of their interdependencies; it simply
%manages a set of tasks that are all individually configured as if they
%were to run in isolation.\footnote{The system manager does of course
%have to ensure that the configured task pool is self consistent, i.e.\
%that each task's prerequisites will be satisfied by some other task(s)
%in the system.}
%The total absence of explicit scheduling logic makes this method
%extremely flexible and extensible.\footnote{To extend the system, one
%simply derives a new class definition that lists the new task's
%prerequisites and outputs. The new task will automatically run at the
%right time, i.e.\ when its prerequisites have been satisfied by some
%other task(s) in the system.}


\subsection{Applicability}

The object oriented dynamic metascheduling concept is quite general and
could in principle be implemented for any set of interdependent tasks.
iCycle, however, is specialized toward cycling forecast systems in that
each task must have an associated {\em forecast reference time} that is
part of a predetermined series for a given task type and is not
necessarily related to the real time at which the task actually runs.  

\subsection{Environment}

EcoConnect operates in a well defined environment so that each real task
knows what its input files look like for a given reference time
(through filenaming conventions) and where to get them from (e.g.\ from
their own input directories, or upstream output directories).
Consequently the control system does not need to know the location of
important input/output files, just (via messaging) that they exist. In a
less structured environment additional tasks could easily be added to
to move files around as needed. 

\subsection{Startup and Initialization}

An initial reference time and list of task object names are read in from
the config file, then each task object is created at the initial
reference time {\em or} at the first subsequent reference time that is
valid for the task type. Optionally, we can tell the controller to
reload the current state dump file (which may have been edited); this
will override the configured start time and task list. After startup,
new tasks are created only by {\em abdication} (below).

An initial run through the {\em task processing} code, by virtue of the
fact that the main loop starts with task processing, causes tasks with
no prerequisites (e.g.\ {\em downloader}) to enter the {\em running}
state and launch their external tasks immediately. Otherwise ({\em or}
if there are no tasks that lack prerequisites) nothing will happen.


\subsection{Task Interaction} 

Each task keeps track of which of its postrequisites are completed, and
asks the other tasks if they can satisfy any of its prerequisites. 

{\small
\noindent
\rule{5cm}{.2mm}
\begin{lstlisting}
class task_pool( Pyro.core.ObjBase ):
    # ...
    def interact( self ):
        # get each task to ask all the others if 
        # they can satisfy its prerequisites
        #--
        for task in self.tasks:
            task.get_satisfaction( self.tasks )
    # ...
\end{lstlisting}
}

\subsection{Running Tasks}

Each task object can launch its associated external task, and enter the
{\em running} state if its prerequisites are all satisfied, any existing
older tasks of the same type are already {\em finished}, and fewer than
{\em MAX\_ RUNAHEAD} finished tasks of the same type still exist (this
stops tasks with no prerequisites from running ahead indefinitely).

\subsection{Pyro Remote Method Calls}

The Pyro request handling loop executes remote method calls coming in
from external tasks, and returns after at least one call was handled.
Pyro must be run in non-default single-threaded mode (see Appendix
\ref{pyro-appendix}).

\subsection{Dumping State} 

The current state (waiting, running, or finished) of each task is
written out to the {\em state dump file}.  This provides a running
snapshot of the system as it runs, and just prior to shutdown or
failure. The controller can optionally start up by loading the state
dump (which can be edited first). Any 'running' tasks are reloaded in
the 'waiting' state.

\appendix

\section{Essential OOP Concepts}

The simplicity of the dynamic scheduling implementation in {\em iCycle}
is critically dependent on the {\em polymorphic} nature of the {\em task
objects} in the program.  This section contains a minimal introduction
to these Object Oriented Programming concepts.  Refer to any OOP
reference for more detail.

\subsection{Classes and Objects}

A {\em class} is essentially a generalisation of {\em data type} to
include {\em behaviour} (i.e.\ functions or {\em methods}) as well as
state.  {\em Objects} are more or less self contained {\em instances} of
a class. For example, a $shape$ class could define a $position$ data
member that describes the location of each shape object, a $move()$
method that causes a shape object to alter its position, and a $draw()$
method that causes it to display itself in the right place on screen.

\subsection{Inheritance}

A {\em derived class} or {\em subclass} inherits the properties (methods
and data members) of a {\em base class}. It can also {\em override}
specific base class properties, or add new properties that aren't
present in the base class. Calling a particular method on an object
invokes the object's own class method if one is defined, otherwise the
immediate base class is searched, and so on down to the root of the
inheritance graph. 

For example, we could derive a $circle$ class from $shape$, adding a
`radius' data member and overriding the $draw()$ to get circle objects
to display themselves as actual circles.  Because we didn't override the
$move()$ method, calling $circle.move()$ would invoke the base class
method, $shape.move()$. 


\subsection{Polymorphism}

Polymorphism is the ability of one type to appear as and be used like
another type.  In OOP languages with inheritance, this usually refers to
the ability to treat derived/sub-class objects as if they were members
of a base class.  In particular, a group of mixed-type objects can all
be treated as members of a common base class. For example, we could
construct a list of $shape$ objects from $circles$, $triangles$, and
$squares$; calling $[list member].draw()$ will invoke the right derived
class $draw()$. This is a very powerful mechanism because {\em it allows
unmodified old code to call new code}: if we later derive an entirely
new kind of shape ($hexagon$, say) with it's own unique behaviour, the
existing program, without modification, will process the new objects in
the proper hexagon-specific way.


\section{Threading in Pyro} \label{pyro-appendix}

With Pyro in {\em single threaded mode}, \verb#handleRequests()# returns
after {\em either} a timeout has occurred {\em or} at least one request
(i.e.\ remote method call) was handled. With \verb#timeout = None# this
allows us to process tasks {\em only} after remote method invocations
come in.  Further, we can detect the remote calls that actually change
task states, and thereby drop into the task processing code only when
necessary, which minimizes non-useful output from the task processing
loop (e.g.\ in dummy mode there are a lot of remote calls on the dummy
clock object, which does not alter tasks at all). 

In {\em multithreaded mode}, \verb#handleRequests()# returns immediately
after creating a new request handling thread for a single remote object
and thereafter remote method calls on that object come in asynchronously
in the dedicated thread. This is not good for the dynamic scheduling
algorithm because tasks are only set running in the task processing
block which can be delayed while \verb#handleRequests()# blocks waiting
for a new connection to be established even as messages that warrent
task processing are coming in on existing connections. The only way
around this is to do task processing on \verb#handleRequests()# timeouts
which results in a lot of unnecessary task processing when nothing
important is happening.


\subsection{Interfacing Tasks To iCycle}
blah.

\subsection{Utility Programs}
blah.

\subsubsection{Configure System}
blah.

\subsubsection{Remote Control}
blah.

\subsubsection{System Monitors}
blah.

\subsection{Logging}
blah.

\section{Miscellaneous Notes}

\subsection{Product Generation}

Note that ``correct scheduling'' is not necessarily equivalent to
``orderly generation of products by reference time'' (the upstream 
forecast model can run continuously regardless of any downstream
processing that depends on it).

\subsection{Catching Up}

The state of ``catching up'' is a property of the individual tasks, not
the whole system. Where this matters, it can be detected by the relevant
external task. E.g.\ if the external topnet(T) task starts up at real
time t greater than the stream flow data time for T (i.e.\ $T+15$ min),
i.e.\ the required stream flow data is already available, then we're
still in catchup. If, on the other hand, the topnet task finds that it
has to wait for its stream flow data time to arrive, then we're caught
up to real time.  This matters because topnet is allowed to run ahead by
a different amount of time depending on whether we're in catchup mode or
not.

\subsection{Controlling Task Execution}

\begin{itemize}
 \item  prerequisites
 \item artificial prerequisites (e.g.\ make nztide depend on nzlam)
 \item delayed instantiation (a task can't run if it doesn't exist yet).
 \item other contraints based on, for example, the number of previous
 instances that still exist in the system, or how far ahead of the 
 slowest/oldest task we can get.
\end{itemize}

\subsection{fuzzy prequisites}

{\em Exact prerequisites} (most tasks): times are specified exactly,
relative to the task's own reference time.  E.g.\ {\em file foo\_{T}.nc
ready} where T is the task's reference time.

{\em Fuzzy prerequisites} (topnet): a time boundary is specified
relative to the task's own reference time; any task with a reference
time greater than or equal to the boundary time can satisify the
prerequisite.

\end{document}
